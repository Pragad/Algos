- READ BLOGS  ON PURE
- SHOW A LOT OF CURIOSITY
- ASK A LOT OF QUESTIONS
- THINK OUT LOUD
- HOW MUCH KNOW ABOUT THE COMPANY
- BODY LANGUAGE
- THOUGHT PROCESS
- Even if you come up with the OPTIMAL SOLUTION (Just give the REASONING behind it)
- HOW AM I APPROACHING THE PROBLEM
- PREPARE QUESTIONS TO ASK
- SEE ABOUT EACH INTERVIEWER AND IF YOU HAVE ANY SIMILARITY

1. Don't Jump into Coding
2. First Do Algo Reasoning
3. Clarify all statements
4. Debugging
5. How you approach the solution

2018:
1. Catch and Throw exception where necessary
    - queue.remove(), queue.add() etc
2. Class design, First come up with design
-----------------------------------------------------
Genral Programming / OOPS Concepts
0. What is Abstraction:
1. What is Encapsulation:
2. What is Polymorphism:
3. Encapsulation vs Abstraction:
3a. How are Abstraction and Encapsulation related
3b. Polymorphism vs Overriding vs Overloading:
3c. Why do you need Polymorphism:
4. Overriding:
5. Overloading:
6. What is Inheritence:
7. Is-A vs Has-A
8. Inheritence Vs Polymorphism:
9. Friend Function vs Static Function
10. External Linkage vs Internal Linkage
11. Early binding vs Late Binding
12. Segmentation Fault:
12b. How to create a Segmentation Fault
13. Interrupts vs Exception
14. Callback Functions:
15. Asynchronous Programming:
16. Allocating in Stack vs Heap:
16b. Is Stack Faster than Heap
17. Memory Allocation on Stack Vs Heap
17b. What and Where are Stack and Heap
18. Boxing and Unboxing
19. Shadowing and Overriding
20. Agile Model
21. Types of Polymorphism
22. Compiler vs Interpretor
23. Recursion vs Iterative Approach
24. Tail Recursion Advantages
25. Problems with Multiple Inheritence
26. Static Linking vs Dynamic Linking
27. How to resolve Hash Collisions
28. Multithreading Best Practices
29. What is Amdhal's law
30. UCS-2 vs UTF-8/16/32
31. When will you use ASCII instead of UTF-8?
32. Number of characters in ASCII
33. Unicode and Character Set
34. How to read a file in reverse
35. How to programmatically cause a core dump in C/C++
36. How to store a password in database
37. Abstract Class vs Interface - When to use each
-----------------------------------------------
UML / DESIGN Questions
0. Associations
1. Parking lot  
2. Coffee Maker
3. Deck of Cards
4. What is Cardinality
5. What are mock Objects
6. UML Diagrams
7. Factory Pattern vs Singleton Pattern
8. MVC PAttern
9. Elevator
10. Bike Shop
-----------------------------------------------
OS NOTES
-1. Every program assumes it has its own memory space
-1b. Memory allocation at compile time
-1c. When we associate '&' with a variable the address we get is the virtual address or the physical address? [duplicate]
-1d. What determines the size of an executable
0. Differentiate Threads and Processes
0b. Explain Multithreading to a 5year old
1. What is a Race Condition
2. Mutex vs Semaphores:
3. Spin Locks vs Mutex:
3a. Mutex Monitor and Semaphore
3b. Spin Lock in a Uniprocessor System
3c. Lock Monitor Mutex and Semaphore
3d. Monitor vs Semaphore
3e. Monitor vs Mutex
3f. Semaphore vs Condition Variable
4. Multi-threaded programming in C++
4b. Thread Join vs Detach
5. Hard Link vs Soft Link
5b. What is an Inode ?
5c. How is data stored? Direct referencing, Double indirect and triple indirect
6. Multi Processor vs Multi Core System
7. What is a File descriptor
8. Why can't RAM be replaced by Cache memory
9. How to avoid deadlocks
10. Latency vs Throughput
11. What is Page Fault
12. Why do we need Paging, Unequal Size Fixed Partitioning and Dynamic Partitioning
12b. If we have infinite memory, then do we still be needing paging
13. What is Virtual Memory
14. Why do we need Virtual Memory
15. Where is Page File / Virtual Memory Stored
16. What is Resident Set Size (RSS) and Virtual Memory Size (VSZ)
17. Paging vs Swapping vs Trashing
18. Why does a 32-bit OS support 4 GB of RAM?
19. MB vs MiB / GB vs GiB
20. SRAM vs DRAM
21. Spurious Wakeups
-----------------------------------------------
NETWORKS
0. How TCP Works:
1. How TCP/IP Network works
1b. How Internet Works
1c. TCP vs UDP
1d. Can two applications listen to the same port
1e. Do browsers listen to port 80?
1f. Do web browsers use different outgoing ports for different tabs?
2. Why is MAC address required    
3. Switch vs Hub vs Router
4. How NTLM Authentication Works
5. How Kerberos Authentication Works
5b. NTLM vs Kerberos
6. What happens when I click the Stop button on the browser
7. what happens when you type in a URL in browser
8. What is DHCP and how it works
9. How WiFi (802.11) works
10. DNS and how it works
11. What is NTP and SNTP
12. What is TLS and SSL
13. What is DHCP
14. Sliding Window Protocol
15. What is LDAP
16. What is Samba
17. What is SMB
18. Ports vs Socket
19. TCP Port and UDP Port
20. IPv6
-----------------------------------------------
SECURITY
1. Asymmetric and Symmetric Encryption
2. What is SSL Termination
-----------------------------------------------
MISC
1. What is REST Servcie
1b. GET vs POST
1c. RPC vs REST
1d. RESTful API is stateless
1e. What exactly is RESTful programming?
1f. IIOP
1g. RMI
2. What is a Websocket
3. Relational Database vs NoSQL Database
4. ACID Properties in DB
5. BASE Properties
6. Tabs vs Spaces as Coding Style
-----------------------------------------------
DISTRIBUTED SYSTEMS:
1. Docker and Kubernetes
2. Memcache, Redis
3. Hadoop
4. Apache Spark
4b. Spark vs MapReduce
5. Hive
6. AWS
6a. Amazon EC2 - Elastic Compute Cloud
7. Kafta
8. RabbitMQ
--------------------------------------------------------------------------------
BEHAVIORAL QUESTIONS
1. Disagree with Manager / Team Member
    Deadline Scheduling
        - Situation:
            - For  a new project my manager initially thought it could be done in a week. I had a disagreement there that we would need 3 weeks for that to completely deliver the product
        - Task
            - Building a system to sync changes between source and target
        - Action
            - I split the project into multiple modules and explain in detail how long each module will take
            - About integration of the modules
            - Performing regression testing on the entire module
            - Also creating a test bed for the environment
        - Result
            - We were able to deliver the product with high quality and on time
            - If we had rushed through in two weeks, we might not have done a thorough
              testing. Might also went with a quicker way to do things instead of going with a scalable way of doing things
    
    Developing Abstract Notification Adapter
        - Situation
            - First make few API calls to Storage subsystem to create a new FPolicy config
            - Later got a separate project which heavily uses the config
            - Didn't have time to make the switch from old to new as it was already out with customers
        - Task
            - Had to create a generic adapter
        - Action
            - Had a knob to switch back to old mechanism in case of any failure
            - Designed the new one in a completely abstract way
        - Result
            - Amazing generic notification adapter

2. Customer Obsession:
    POC Example where we failed to configure on storage systems
    I have worked very closely with customers
    How customers would configure / unconfigure
        - Where they would like to select multiple entries and perform "enable all" / "disable all"
    Multi-node cluster setup

3. Highest Standards:
    - Coding standards
        - Comments
        - Documentation etc

4. Ownership:
    - Own NetApp and EMC storage systems

--------------------------------------------------------------------------------
GENRAL PROGRAMMING / OOPS Concepts
0.
What is Abstraction:
    - Abstraction is the concept of ignoring unnecessary details.
    - Or other words, describing something in a simpler way by not concentrating on the intricate details and focusing only on necessary details.
    - It can be thought as generalization.
    - Say when you are meeting a random person, you would say that you are the guy in Black Shirt and Blue Jean.
    - We can consider Interfaces as Abstraction.

1.
What is Encapsulation:
    - Encapsulation is something like group similar things in a box.
    - So someone else won't have access to the inside of the box. They can just use the box to do what they want.
    - Changing the contents of the box won't affect the outsider. They will still be able to use the box.

2.
What is Polymorphism:
    - Polymorphism is the ability to an object to decide what it should.
    - Simple example would be to consider "Shapes" class. And the class has methods like Area and Cirumferance.
    - We can have subclasses like Square, Rectangle.
    - To find the area of all the shapes, we can just give Shapes.area(). Based on what shape it is, we will get the respective Area.

3. Encapsulation vs Abstraction:
VERY IMP:
http://stackoverflow.com/questions/12072980/encapsulation-vs-abstraction-real-world-example
http://blogs.siliconindia.com/deepvijay/Technology/Encapsulation-and-Abstraction-a-real-time-example-bid-wvGh8f5F89208310.html


        Encapsulation puts some things in a box and gives you a peephole; this keeps you from mucking with the gears.

        Abstraction is the process of generalization: taking a concrete implementation and making it applicable to different, albeit somewhat related, types of data. The classical example of abstraction is C's qsort function which sorts data.


        Encapsulation:-- Information hiding.
        Abstraction:-- Implementation hiding.

            Example:

            class foo{
                private:
                    int a, b;
                public:
                    foo(int x=0, int y=0): a(x), b(y) {}

                    int add(){    
                        return a+b;   
                    } 
            }  
        Internal representation of any object of foo class is hidden outside the class. --> Encapsulation.
        Any accessible member (data/function) of an object of foo is restricted and can only be accessed by that object only.

            foo foo_obj(3, 4);
            int sum = foo_obj.add();
        Implementation of method add is hidden. --> Abstraction.


        Abstraction:
            The thing about qsort is that it doesn't care about the data it sorts – in fact, it doesn't know what data it sorts. Rather, its input type is a typeless pointer (void*) which is just C's way of saying “I don't care about the type of data” (this is also called type erasure). The important point is that the implementation of qsort always stays the same, regardless of data type. The only thing that has to change is the compare function, which differs from data type to data type. qsort therefore expects the user to provide said compare function as a function argument.


    Abstraction is achieved by making class abstract having one or more methods abstract.
    Which is nothing but essential characteristic which should be implemented by the class extending it.
    e.g. when you inventing/designing a car you define a characteristics like car should have 4 doors, break, steering wheel etc… so anyone uses this design should include this characteristics.
    Encapsulation:
        Example 1.
        A cell phone is a great example.
        I have no idea how the cell phone connects to a satellite, tower, or another phone.
        I have no idea how the damn thing understands my key presses or how it takes and sends pictures to an email address or another phone number.
        I have no idea about the intricate details of most of how a modern smart phone works.

        Example 2.
        As a driver you know how to start the car by pressing the start button and internal details of the starting operations are hidden from you.
        So the entire starting process is hidden from you otherwise we can tell starting operation is encapsulated from you.

    Abstraction:
        Example 1.
        But, I can use a cell phone.
        The phones have standard interfaces (yes - both literal and software design) that allows someone who understand the basics of one to use almost all of them.

        Example 2.
        The car's operation is completely abstracted from you and it is partially implemented to Local Mechanic Entity and fully implemented to Expert Entity.
        So you are an abstract class having only abstract methods, Local Mechanic Entity has extended You(Since he is also an ordinary user) and he implemented some of the methods and last our expert Entity extending Local Mechanic and implementing all the methods

        Though every method is an encapsulation, it is also an abstraction, because every time you put some things together and give it a name you create a new (abstract) concept. Encapsulation without abstraction is useless.

    http://stackoverflow.com/questions/742341/difference-between-abstraction-and-encapsulation

        Abstraction flat-out ignores the details that don't matter, like whether the things have gears, ratchets, flywheels, or nuclear cores; they just "go"

        Abstraction is hiding the implementation details by providing a layer over the basic functionality.

INFORMATION HIDING, ENCAPSULATION and ABSTRACTION:
        Information Hiding is hiding the data which is being affected by that implementation. Use of private and public comes under this. For example, hiding the variables of the classes.

        Encapsulation is just grouping all similar data and functions into a group e.g Class in programming; Packet in networking.

        - Putting data and behavior together is what I understand from encapsulation.
        - Hiding complexity is abstraction.
        - Information should remain unseen is what information hiding is. Very famous three sentences.

    http://stackoverflow.com/questions/742341/difference-between-abstraction-and-encapsulation
        Most answers here focus on OOP but encapsulation begins much earlier; every method is an encapsulation:

        point x = { 1, 4 };
        point y = { 23, 42 };

        int d = distance(x, y);

        Here, distance encapsulates the calculation of the (euclidean) distance between two points in a plane: it hides implementation details. This is encapsulation, pure and simple.
 
3a.
How are Abstraction and Encapsulation related
http://stackoverflow.com/questions/12072980/encapsulation-vs-abstraction-real-world-example
    Both abstraction and encapsulation are underlying foundations of object oriented thought and design.
    So, in our cell phone example.
    The notion of a smart phone is an abstraction, within which certain features and services are encapsulated.
    The iPhone and Galaxy are further abstractions of the higher level abstraction.
    Your physical iPhone or Galaxy are concrete examples of multiple layers of abstractions which contain encapsulated features and services.

3b.
Polymorphism vs Overriding vs Overloading:
    http://stackoverflow.com/questions/154577/polymorphism-vs-overriding-vs-overloading

    The clearest way to express polymorphism is via an abstract base class (or interface)

    public abstract class Human{
       ...
       public abstract void goPee();
    }

    This class is abstract because the goPee() method is not definable for Humans.
    It is only definable for the subclasses Male and Female.
    Also, Human is an abstract concept - You cannot create a human that is neither Male nor Female. It’s got to be one or the other.

    So we defer the implementation by using the abstract class.
        public class Male extends Human{
        ...
            @Override
            public void goPee(){
                System.out.println("Stand Up");
            }
        }

        public class Female extends Human{
        ...
            @Override
            public void goPee(){
                System.out.println("Sit Down");
            }
        }

    Now we can tell an entire room full of Humans to go pee.
        public static void main(String[] args){
            ArrayList<Human> group = new ArrayList<Human>();
            group.add(new Male());
            group.add(new Female());
            // ... add more...

            // tell the class to take a pee break
            for (Human person : group) person.goPee();
        }

3c.
Why do you need Polymorphism:
    Say you have MS Company. It creates a Function.

    Google, FB, etc are derived classes whose object will be passed to the function that MS has.
    It will of the form,
    MS ob1 = new Google();
    MS ob2 = new FB();

    MS can just execute it by doing ob.exec();

4.
Overriding:
    - Concept of adding more details to a function in the derived class.
    - It replaces the function in the base class.

5.
Overloading:
    - It is concept of defining multiple methods based on the parameters passed.

6.
What is Inheritence:
    - It is concept where a new class is derived from an existing class.
    - This happens when we want to have a new Class that will make use of an existing class with some added details to it.
    - Eg we have a Vehicle / Shapes class and we want to add Car / Square class.
    - Inheritence should only be used if the sub class is BEHAVIORIALLY equal to the superclass.

7.
Is-A vs Has-A
    A House is a Building (inheritance);
    A House has a Room (composition);
    A House has an occupant (aggregation).
    http://stackoverflow.com/questions/49002/prefer-composition-over-inheritance
        
        Does TypeB want to expose the complete interface (all public methods no less) of TypeA such that TypeB can be used where TypeA is expected? Indicates Inheritance.
        e.g. A Cessna biplane will expose the complete interface of an airplane, if not more. So that makes it fit to derive from Airplane.

        Does TypeB only want only some/part of the behavior exposed by TypeA? Indicates need for Composition.
        e.g. A Bird may need only the fly behavior of an Airplane. In this case, it makes sense to extract it out as an interface / class / both and make it a member of both classes.

    http://enoshtechdiary.blogspot.com/2012/04/composition-vs-aggregation.html

    Usage of Inheritence and Aggregation
    - If we have used inheritance but we only use part of the interface, or we are forced to override a lot of functionality to keep the correlation logical.
      Then we have a big nasty smell that indicates that we had to use aggregation.
    - If we have used aggregation (or we plan to use it) but we find out we need to copy almost all of the functionality. Then we have a smell that points in the direction of inheritance.

    HAS-A - ASSOCIATION
    1. Composition:
        - REPRESENTED using FILLED DIAMOND in a CLASS DIAGRAM
        - Composition is an association in which one class CANNOT exisit without other.
        - EG: CAR has-a Engine.
        - When we destroy CAR, we destroy Object as well.
        - CAR manages the lifetime of the Engine.

        - OUR TREE STURUCTURE has FILES.
        - When we destroy Trees, we destroy Files.

    2. AGGREGATION
        - REPRESENTED using EMPTY DIAMOND in a CLASS DIAGRAM
        - Aggregation is a Composition in which one class belongs to a Collection.
        - However it CAN exist without the WHOLE.
        - EG: CAR has a DRIVER.

        - Multichannel Sessions that can exist even if the Connection goes away.

8. Inheritence Vs Polymorphism:
    http://stackoverflow.com/questions/6308178/what-is-the-main-difference-between-inheritance-and-polymorphism
    http://www.tutorialspoint.com/cplusplus/cpp_inheritance.htm

    // Base class
    class Shape 
    {
       public:
          void setWidth(int w)
          {
             width = w;
          }
          void setHeight(int h)
          {
             height = h;
          }
       protected:
          int width;
          int height;
    };

    // Derived class
    class Rectangle: public Shape
    {
       public:
          int getArea()
          { 
             return (width * height); 
          }
    };

    int main(void)
    {
       Rectangle Rect;
     
       Rect.setWidth(5);
       Rect.setHeight(7);

       // Print the area of the object.
       cout << "Total area: " << Rect.getArea() << endl;

       return 0;
    }

9.
Friend Function vs Static Function
http://stackoverflow.com/questions/4723143/static-member-functions
http://stackoverflow.com/questions/2315166/where-would-you-use-a-friend-function-vs-a-static-member-function
http://stackoverflow.com/questions/4921150/when-to-use-static-member-function?lq=1

    Static Variable:
        Lifetime is throughout the program. It will hold valid for all instances of the Class.
    Static Function:
        PRotected static function.
        Good uses of static member functions:
            Meta-programming: writing code that writes code; Metaprogramming refers to a variety of ways a program has knowledge of itself or can manipulate itself.
                
            Real-world example is template std::char_traits. All member functions are static
            Making it a static member function gives it access to private members of the class, although a friend would suffice here too
            A protected static member function thus is accessible only to the class and classes derived from it.

http://stackoverflow.com/questions/2671496/java-when-to-use-static-methods
    Ask yourself "does it make sense to call this method, even if no Obj has been constructed yet?" If so, it should definitely be static.

    So in a class Car you might have a method double convertMpgToKpl(double mpg) which would be static, because one might want to know what 35mpg converts to, even if nobody has ever built a Car.
    But void setMileage(double mpg) (which sets the efficiency of one particular Car) can't be static since it's inconceivable to call the method before any Car has been constructed.

    Define static methods in the following scenarios only:
        1. If you are writing utility classes and they are not supposed to be changed.
        2. If the method is not using any instance variable.
        3. If any operation is not dependent on instance creation.
        4. If there is some code that can easily be shared by all the instance methods, extract that code into a static method.
        5. If you are sure that the definition of the method will never be changed or overridden. As static methods can not be overridden.

    Friend
        In many situations you will have objects with data or functionality that are not intended to be called publicly available.

    Friend function:
        It has access to private members of the class.

http://stackoverflow.com/questions/17434/when-should-you-use-friend-in-c
    Friend CLASS:
        The 'friend' specifier allows the designated class access to protected data or functionality within the class making the friend statement.
        For example in the below code anyone may ask a child for their name, but only the mother and the child may change the name.

        You can take this simple example further by considering a more complex class such as a Window. Quite likely a Window will have many function/data elements that should not be publicly accessible, but ARE needed by a related class such as a WindowManager.

        class Child
        {
        friend class Mother;

        public:

          string name( void );

        protected:

          void setName( string newName );
        };

10.
External Linkage vs Internal Linkage
    TIC++
    - Internal linkage means that storage is created to represent the identifier only for the file being compiled.
    - Internal linkage is specified by the keyword static in C and C++

    Const defaults to Internal Linkage in C++ and External Linkage in C

    - External linkage means that a single piece of storage is created to represent the identifier for all files being compiled.
    - The storage is created once, and the linker must resolve all other references to that storage.
    - Global variables and function names have external linkage.
    - These are accessed from other files by declaring them with the keyword extern.

11.
Early binding vs Late Binding
    a. Early Binding:
       In Method Overloading your method calls to the methods are decided by the compiler in the sense that which function is going to be called is decided by your compiler at compile time. Hence being EARLY BINDING
       Compiler generates a call to a specific function
       Runtime system resolves this call to aboslute address of the code

    b. Late Binding:
       In method Overriding, it is decided at RUNTIME which method is going to be called. So it is reffered as LATE BINDING.
       Compiler just ensures that the method exists and performs type checking on args and return value
       Compiler DOES NOT know the exact code to execute
       - Address of the method is calculated USING the information stored in the object.

12.
Segmentation Fault:
    http://stackoverflow.com/questions/2346806/what-is-segmentation-fault

    - generates SIGSEGV event
    - Segmentation fault is a specific kind of error caused by accessing memory that “does not belong to you.
            int *p = NULL;
            *p = 1;

        Another segfault happens when you try to write to a portion of memory that was marked as read-only:
            char *str = "Foo"; // Compiler marks the constant string as read-only
            *str = 'b'; // Which means this is illegal and results in a segfault

    It would be worth noting that segmentation fault isn't caused by directly accessing another process memory, as it is simply not possible.
    With virtual memory every process has its own virtual address space and there is no way to access another one using any value of pointer.
    Exception to this can be shared libraries which are same physical address space mapped to (possibly) different virtual addresses and kernel memory which is even mapped in the same way in every process (to avoid TLB flushing on syscall, I think).  - these are what I count as 'indirect' access.
    One can, however, check that they are usually located long way from process code and we are usually able to access them (this is why they are there, nevertheless accessing them in a improper way will produce segmentation fault).

    Still, SEGMENTATION FAULT CAN OCCUR IN CASE OF ACCESSING OUR OWN (PROCESS) MEMORY IN IMPROPER WAY (for instance trying to write to non-writable space).
    But the most common reason for it is the access to the part of the virtual address space that is not mapped to physical one at all.

12b.
How to create a Segmentation Fault
http://stackoverflow.com/questions/2045314/why-cant-i-cause-a-seg-fault

    It impossable to try and reliable do it dereferencing pointers.
    This is because how the application handles memory can vary from compiler to compiler also across the same compiler with different options (debug/release mode handeled differently).

    What you can do is explicitly raise the segfault using a signal:
        #include <signal.h>
        int main()
        {
            raise(SIGSEGV);
        }

13.
Interrupts vs Exception
    http://stackoverflow.com/questions/125394/interrupts-and-exceptions

    WHERE AN EXCEPTION DIFFERS FROM AN INTERRUPT IS THAT AN EXCEPTION IS CAUSED BY SOME ILLEGAL ACTIVITY THAT THE PROCESSOR HAS DETECTED.

    Interrupt:
        Interrupts are generated by devices external to the CPU (timer tick, disk operation completion, network packet arrival, etc.)
        An interupt is a CPU signal generated by hardware, or specific CPU instructions.
        These cause interupt handlers to be executed.
        Things such as I/O signals from I/O hardware generate interupts.
        Eg: Keyboard Interrupt

        Your processor is going to have a number of external interrupt pins.
        Typically these pins are connected to hardware and are used to indicate when some external event occurs.

        Other peripherals like timers, usb controllers, etc. will also generate interrupts on the basis of some external event.

    ISR:
        When the processor receives a signal on one of it's external interrupt pins it will immediately jump to some nominated location in memory and start executing.
        The code executed is typically called an ISR, or interrupt service routine.

    TIMER INTERRUPT:
        So, let's say I write a simple C program that counts all numbers, or the Fibonacci sequence, or something else without stopping. Or even better: does nothing but spins inside of a while(1) loop. How do the other processes on the system get a chance to run? What if there is nothing happening to cause an interrupt?

        The answer is that you have a timer device that is constantly interrupting. And it is what keeps a spinning process from taking down the entire system.

    Exception:
        An exception can be thought of as a software-version of an interupt, that only affects its process.

    There have been 3 different meanings listed in other answers on this page.

    SOFTWARE CONSTRUCT:
        This is purely an application level exception, where a piece of code is able to indicate an error that can be detected by some other piece of code.
        There is no hardware involvement here at all.

    TASK EXCEPTION:
        Then there is the exception as seen by a task.
        This is an operating system level construct that is used to kill a task when it does something illegal - like divide by 0, illegally accessing memory etc.

    HARDWARE EXCEPTION:
        And thirdly, there is the hardware exception.
        In terms of behaviour it is identical to an interrupt in that the processor will immediately jump to some nominated memory location and start executing.
        Where an exception differs from an interrupt is that an exception is caused by some illegal activity that the processor has detected.
        For example the MMU on the processor will detect illegal memory accesses and cause an exception.
        These hardware exceptions are the initial trigger for the operating system to perform it's cleanup tasks (as described in the paragraph above)

14.
Callback Functions:
    http://stackoverflow.com/questions/9596276/how-to-explain-callbacks-in-plain-english-how-are-they-different-from-calling-o
        Imagine this scenario: You are expecting a package in a couple of days. The package is a gift for your neighbor. Therefore, once you get the package, you want it brought over to the neighbors. You are out of town, and so you leave instructions for your spouse.

        You could tell them to get the package and bring it to the neighbors. If your spouse was as stupid as a computer, they would sit at the door and wait for the package until it came (NOT DOING ANYTHING ELSE) and then once it came they would bring it over to the neighbors. But there's a better way. Tell your spouse that ONCE they receive the package, they should bring it over the neighbors. Then, they can go about life normally UNTIL she receives the package.

        In our example, the receiving of the package is the "event" and the bringing it to the neighbors is the "callback". Your spouse "runs" your instructions to bring the package over only when the package arrives. Much better!

        This kind of thinking is obvious in daily life, but computers don't have the same kind of common sense. Consider how programmers normally write to a file:

        fileObject = open(file)
        # now that we have WAITED for the file to open, we can write to it
        fileObject.write("We are writing to the file.")
        # now we can continue doing the other, totally unrelated things our program does

        Here, we WAIT for the file to open, before we write to it. This "blocks" the flow of execution, and our program cannot do any of the other things it might need to do! What if we could do this instead:

        # we pass writeToFile (A CALLBACK FUNCTION!) to the open function
        fileObject = open(file, writeToFile)
        # execution continues flowing -- we don't wait for the file to be opened
        # ONCE the file is opened we write to it, but while we wait WE CAN DO OTHER THINGS!

15.
Asynchronous Programming:
    In solving many engineering problems, the software is designed to split up the overall problem into multiple individual tasks, and then execute them asynchronously.
    Inverting a matrix, or a finite element analysis problem, are good examples.
    In computing, sorting a list is an example. The quick sort routine, for example, splits the list into two lists, and sorts each of them by calling itself recursively.
    In both of the above examples, the two tasks can (and often were) executed asynchronously.
    They do not need to be on separate threads.
    Even a machine with one CPU, and only one thread of execution can be coded to initiate processing of a second task before a first one has completed.
    THE ONLY CRITERION IS THAT THE RESULTS OF ONE TASK ARE NOT NECESSARY AS INPUTS TO THE OTHER TASK.
    As long as the start and end times of the tasks overlap, (possible only if the output of neither is needed as inputs to the other), they are being executed asynchronously, no matter how many threads are in use.

16.
Allocating in Stack vs Heap:
    STACK:
        SCOPE - Function
        Allocating in the stack is easy and fast, but stack is limited,
        Apart from that, stack allocated values are "deleted" once you leave the scope, so it is very good for small local values like primitive variables.

        If you allocate too much in the stack you might run out of stack and die,
        Main as all the functions you execute has a stack frame in the stack and all the local variables to the function are stored there.
        So going too deep into function calling might get you into a stackoverflow as well.

        In general is a good rule of thumb to allocate anything that you use often and is bigger than a hundred bytes in the heap, and small variables and pointers in the stack.

    HEAP:
        SCOPE - When you want something to be used outside of the function
        heap is slower but much bigger.
        In general is a good rule of thumb to allocate anything that you use often and is bigger than a hundred bytes in the heap,

16b.
Is Stack Faster than Heap
http://stackoverflow.com/questions/161053/which-is-faster-stack-allocation-or-heap-allocation
http://stackoverflow.com/questions/24057331/is-accessing-data-in-the-heap-faster-than-from-the-stack

    Stack is always hot, the memory you get is much more likely to be in cache than any far heap allocated memory

    Stack allocation is much faster since all it really does is move the stack pointer.
    Using memory pools, you can get comparable performance out of heap allocation

    Stack is much faster. It literally only uses a single instruction on most architectures, in most cases, e.g. on x86:
    (That moves the stack pointer down by 0x10 bytes and thereby "allocates" those bytes for use by a variable.)

        sub esp, 0x10

    The OS (which is responsible for page faulting / swapping), and the hardware (CPU) trapping on accesses to swapped-out or not-yet-accessed pages, would not even be tracking which pages are "stack" vs "heap"... a memory page is a memory page

    That said, the virtual address of global data may be able to be calculated and hardcoded at compile time,
    The addresses of stack-based data are typically stack-pointer relative, while memory on the heap must almost always be accessed using pointers, which might be slightly slower on some systems -
    it depends on the CPU addressing modes and cycles, but it's almost always insignificant -
    not even worth a look or second thought unless you're writing something where millionths of a second are enormously important.

17.
Memory Allocation on Stack Vs Heap
http://stackoverflow.com/questions/2264969/why-is-memory-allocation-on-heap-much-slower-than-on-stack

    Stack Allocation:
        - Just a matter of changing the stack pointer
        - Just one Instruction
            EG: sub esp, 0x10 - moves stack pointer by 10 bytes.
        - Stack Allocations are also scoped to a particular function.
        - Stack uses Data Locality and Caches the TOP of stack
        - TOP of Stack is ALMOST ALWAYS on Cache Line
        - Stack Variables might also get stored in Registers

        - Items on Stack CANNOT be removed out of order.
        - So adding a new items just adds to the end of stack. Move one pointer in stack.

    Heap Allocation:
        - Allocating memory on the heap involves looking for a big enough block, splitting it, and managing the "book-keeping" that allows things like free() in a different order.
        - On Heap, Elements can be removed OUT of ORDER. So tough to maintain as HOLES will
          be in middle.

17b.
What and Where are Stack and Heap
http://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap?lq=1

    The stack is the memory set aside as scratch space for a thread of execution.
    When a function is called, a block is reserved on the top of the stack for local variables and some bookkeeping data.
    When that function returns, the block becomes unused and can be used the next time a function is called.
    The stack is always reserved in a LIFO (last in first out) order; the most recently reserved block is always the next block to be freed.
    This makes it really simple to keep track of the stack; freeing a block from the stack is nothing more than adjusting one pointer.

    The heap is memory set aside for dynamic allocation.
    Unlike the stack, there's no enforced pattern to the allocation and deallocation of blocks from the heap; you can allocate a block at any time and free it at any time.
    This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time; there are many custom heap allocators available to tune heap performance for different usage patterns.

    Each thread gets a stack, while there's typically only one heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation).

    To answer your questions directly:
    Q: To what extent are they controlled by the OS or language runtime?
    A: The OS allocates the stack for each system-level thread when the thread is created.
       Typically the OS is called by the language runtime to allocate the heap for the application.

    Q: What is their scope?
    A: The stack is attached to a thread, so when the thread exits the stack is reclaimed.
       The heap is typically allocated at application startup by the runtime, and is reclaimed when the application (technically process) exits.

    Q: What determines the size of each of them?
    A: The size of the stack is set when a thread is created.
       The size of the heap is set on application startup, but can grow as space is needed (the allocator requests more memory from the operating system).

    Q: What makes one faster?
    A: The stack is faster because the access pattern makes it trivial to allocate and deallocate memory from it (a pointer/integer is simply incremented or decremented), while the heap has much more complex bookkeeping involved in an allocation or deallocation.
       Also, each byte in the stack tends to be reused very frequently which means it tends to be mapped to the processor's cache, making it very fast.
       Another performance hit for the heap is that the heap, being mostly a global resource, typically has to be multi-threading safe, i.e. each allocation and deallocation needs to be - typically - synchronized with "all" other heap accesses in the program.

18.
Boxing and Unboxing
http://stackoverflow.com/questions/2111857/why-do-we-need-boxing-and-unboxing-in-c
    
    Language C#:
        double e = 2.718281828459045;
        object o = e; // box
        int ee = (int)(double)o; //unbox

    First we have to explicitly unbox the double ((double)o) and then cast that to an int
    
19.
Shadowing and Overriding
http://stackoverflow.com/questions/392721/difference-between-shadowing-and-overriding-in-c

    class A
    {
        public int Foo(){ return 5;}
        public virtual int Bar(){return 5;}
    }
    class B : A{
        public new int Foo() { return 1;}     //shadow
        public override int Bar() {return 1;} //override
    }

    then when you call this:
        A clA = new A();
        B clB = new B();

        Console.WriteLine(clA.Foo()); // output 5
        Console.WriteLine(clA.Bar()); // output 5
        Console.WriteLine(clB.Foo()); // output 1
        Console.WriteLine(clB.Bar()); // output 1

        //now let's cast B to an A class
        Console.WriteLine(((A)clB).Foo()); // output 5 <<<--
        Console.WriteLine(((A)clB).Bar()); // output 1

20.
Agile Model
http://istqbexamcertification.com/what-is-agile-model-advantages-disadvantages-and-when-to-use-it/
   - Software is developed in incremental, rapid cycles. results in small incremental releases with each release building on previous functionality 
   - DIS ADV: In case of some software deliverables, especially the large ones, it is difficult to assess the effort required at the beginning of the software development life cycle.

21.
Types of Polymorphism
    Run time - Overriding
    Compile time - Over loading

22.
Compiler vs Interpretor
http://stackoverflow.com/questions/3996651/what-is-compiler-linker-loader

    Compiler:
        Takes a high level language and converts it into Machine Code
        The machine code is put in memory for execution

    Interpretor:
        Take the high level language code and on the go put each line into memory for execution
        Doesn't get converted into an intermediate machine code.
        But it does it at the moment the program is run

    Compiler characteristics:
        spends a lot of time analyzing and processing the program
        the resulting executable is some form of machine- specific binary code
        the computer hardware interprets (executes) the resulting code
        program execution is fast

    Interpreter characteristics:
        relatively little time is spent analyzing and processing the program
        the resulting code is some sort of intermediate code
        the resulting code is interpreted by another program
        program execution is relatively slow

    =====> COMPILATION PROCESS <======
                         |
                         |---->  Input is Source file(.c)
                         |
                         V
                +=================+
                |                 |
                | C Preprocessor  |
                |                 |
                +=================+
                         |
                         | ---> Pure C file ( comd:cc -E <file.name> )
                         |
                         V
                +=================+
                |                 |
                | Lexical Analyzer|
                |                 |
                +-----------------+
                |                 |
                | Syntax Analyzer |
                |                 |
                +-----------------+
                |                 |
                | Semantic Analize|
                |                 |
                +-----------------+
                |                 |
                | Pre Optimization|
                |                 |
                +-----------------+
                |                 |
                | Code generation |
                |                 |
                +-----------------+
                |                 |
                | Post Optimize   |
                |                 |
                +=================+
                         |
                         |--->  Assembly code (comd: cc -S <file.name> )
                         |
                         V
                +=================+
                |                 |
                |   Assembler     |
                |                 |
                +=================+
                         |
                         |--->  Object file (.obj) (comd: cc -c <file.name>)
                         |
                         V
                +=================+
                |     Linker      |
                +=================+
                         |
                         |--->  Executable (.Exe/a.out) (com:cc <file.name> ) 
                         |
                         V
                Executable file(a.out)
                         |
                         |---> Put into Main Memory
                         |
                +=================+
                |     Loader      |
                +=================+

23.
Recursion vs Iterative Approach
http://www.codeproject.com/Articles/21194/Iterative-vs-Recursive-Approaches
http://stackoverflow.com/questions/72209/recursion-or-iteration

    It is possible that recursion will be more expensive, depending on if the recursive function is tail recursive (last line is recursive call).
    Tail recursion should be recognized by the compiler and optimized to its iterative counterpart (while maintaining the concise, clear implementation you have in your code).

    Basically, a recursive algorithm will add overhead since you store recursive calls in the execution stack.

    But if the recursive function is the last line of the call (tail recursion) then there is no additional penalty.

    That is of course both algorithms are the same

24.
Tail Recursion Advantages
http://stackoverflow.com/questions/19854007/what-is-the-advantage-of-using-tail-recursion-here

    A tail recursive function call allows the compiler to perform a special optimization which it normally can not with regular recursion.
    In a tail recursive function, the recursive call is the very last thing to be executed.
    In this case, instead of allocating a stack frame for each call, the compiler can rework the code to simply reuse the current stack frame, meaning a tail-recursive function will only use a single stack frame as opposed to hundreds or even thousands.

Not tail recursive example:
    // A NON-tail-recursive function.  The function is not tail
    // recursive because the value returned by fact(n-1) is used in
    // fact(n) and call to fact(n-1) is not the last thing done by fact(n)
    unsigned int fact(unsigned int n)
    {
        if (n == 0) return 1;
     
        return n*fact(n-1);
    }

Tail Recursive example:
    // A tail recursive function to calculate factorial
    unsigned factTR(unsigned int n, unsigned int a)
    {
        if (n == 0)  return a;
     
        return factTR(n-1, n*a);
    }
     
    // A wrapper over factTR
    unsigned int fact(unsigned int n)
    {
       return factTR(n, 1);
    }

25.
Problems with Multiple Inheritence
http://stackoverflow.com/questions/225929/what-is-the-exact-problem-with-multiple-inheritance
http://stackoverflow.com/questions/406081/why-should-i-avoid-multiple-inheritance-in-c

    The most obvious problem is with function overriding.

    Let's say have two classes A and B, both of which define a method "doSomething".
    Now you define a third class C, which inherits from both A and B, but you don't override the "doSomething" method.

    When the compiler seed this code...
        C c = new C();
        c.doSomething();

    ...which implementation of the method should it use?
    Without any further clarification, it's impossible for the compiler to resolve the ambiguity.

    Besides overriding, the other big problem with multiple inheritance is the layout of the physical objects in memory.

    Languages like C++ and Java and C# create a fixed address-based layout for each type of object. Something like this:

        class A:
            at offset 0 ... "abc" ... 4 byte int field
            at offset 4 ... "xyz" ... 8 byte double field
            at offset 12 ... "speak" ... 4 byte function pointer

        class B:
            at offset 0 ... "foo" ... 2 byte short field
            at offset 2 ... 2 bytes of alignment padding
            at offset 4 ... "bar" ... 4 byte array pointer
            at offset 8 ... "baz" ... 4 byte function pointer

    When the compiler generates machine code (or bytecode), it uses those numeric offsets to access each method or field.

    Multiple inheritance makes it very tricky.

    If class C inherits from both A and B, the compiler has to decide whether to layout the data in AB order or in BA order.

    But now imagine that you're calling methods on a B object.
    Is it really just a B? Or is it actually a C object being called polymorphically, through its B interface? Depending on the actual identity of the object, the physical layout will be different, and its impossible to know the offset of the function to invoke at the call-site.

    The way to handle this kind of system is to ditch the fixed-layout approach, allowing each object to be queried for its layout before attempting to invoke the functions or access its fields.

    it's a pain in the neck for compiler authors to support multiple inheritance.
    So when someone like Guido van Rossum designs python, or when Anders Hejlsberg designs c#, they know that supporting multiple inheritance is going to make the compiler implementations significantly more complex, and presumably they don't think the benefit is worth the cost.

    Summary

        1. Consider composition of features, instead of inheritance
        2. Be wary of the Diamond of Dread
        3. Consider inheritance of multiple interfaces instead of objects
        4. Sometimes, Multiple Inheritance is the right thing. If it is, then use it.
        5. Be prepared to defend your multiple-inherited architecture in code reviews

    1. Perhaps composition?
        This is true for inheritance, and so, it's even more true for multiple inheritance.

        Does your object really needs to inherit from another?
        A Car do not need to inherit from an Engine to work, nor from a Wheel.
        A Car has an Engine and four Wheel.

        If you use multiple inheritance to resolve this problem instead of composition, then you did something wrong.

    2. The Diamond of Dread
        Usually, you have a class A, and then B and C both inherit from A.
        And don't ask me why, someone then decides that D must inherit both from B and C.

        I encountered this kind of problem twice in 8 eights years, and it is amusing to see because of:

            1. How much a mistake it was from the beginning (In both cases, D should not have inherited from both B anc C), because this was bad architecture (in fact, C should not have existed at all...)
            2. How much maintainers were paying for that, because in C++, the parent class A was present twice in its grandchild class D, and thus, updating one parent field A::field meant either updating them twice (through B::field and C::field), or have something goes silently wrong and crash, later (new a pointer in B::field, and delete C::field...)

        Using the keyword virtual in C++ void the double layout described above, but anyway, you're probably doing something wrong...

        In Object hierarchy, you should keep the hiearchy as a Tree (a node has ONE parent), not as a graph.

    3. Interfaces

        Multiple inheritance of zero or one concrete classe, and zero or more interfaces is usually Ok, because you won't encounter the Diamond of Dread described above.
        In fact, this is how things are done in Java.

        Usually, what you mean when C inherits from A and B is that users can use C as if it was a A, and/or as if it was a B.

        In C++, an interface is an abstract class which has:
            1. all its method declared pure virtual (suffixed by = 0)
            2. no member variables

        The Multiple inheritance of zero to one real object, and zero or more interfaces is not considered "smelly" (at least, not as much)

    4. Do you really need Multiple Inheritance?
        Sometimes, yes.

        Usually, your C class is inheriting from A and B, and A and B are two unrelated objects (i.e. not in the same hierarchy, nothing in common, different concepts, etc.).

        For example, you could have a system of Nodes with X,Y,Z coordinates, able to do a lot of geometric calculations (perhaps a point, part of geometric objects) and each Node is an Automated Agent, able to communicate with other agents.

        Perhaps you already have access to two libraries, each with its own namespace (another reason to use namespaces... But you use namespaces, don't you?), one being "geo" and the other being "ai"

        So you have your own "own::Node" derive both from "ai::Agent" and "geo::Point".

        This is the moment when you should ask yourself if you should not use composition instead. If "own::Node" is really really both a "ai::Agent" and a "geo::Point", then composition would not do.

        Then you'll need multiple inheritance, having your "own::Node" communicate with other agents according to their position in a 3D space.

        (You'll note that ai::Agent and geo::Point are completely, totally, fully UNRELATED... This drastically reduces the danger of multiple inheritance)

    5. So, should I do Multiple Inheritance?
        Most of the time, no. MI is not the right tool, even if it seems to work.

        But sometimes, yes. And at that time, nothing will work better than MI, and you won't have a Diamond of Dread, and your object is really both its parent.

        But because MI is smelly, be prepared to defend your architecture in code reviews (and defending it is a good thing, because if you're not able to defend it, then you should not do it).

26.
Static Linking vs Dynamic Linking
https://kb.iu.edu/d/akqn
http://stackoverflow.com/questions/311882/what-do-statically-linked-and-dynamically-linked-mean
http://cs-fundamentals.com/tech-interview/c/difference-between-static-and-dynamic-linking.php

    Statically-linked files are 'locked' to the executable at link time so they never change.
    A dynamically linked file referenced by an executable can change just by replacing the file on the disk.

    This allows updates to functionality without having to re-link the code;
    the loader re-links every time you run it.

    This is both good and bad - on one hand, it allows easier updates and bug fixes,
    on the other it can lead to programs ceasing to work if the updates are incompatible - this is sometimes responsible for the dreaded "DLL hell" that some people mention in that applications can be broken if you replace a dynamically linked library with one that's not compatible (developers who do this should expect to be hunted down and punished severely, by the way).

    When you compile some C code (for instance), it is translated to machine language. Just a sequence of bytes which, when run, causes the processor to add, subtract, compare, "goto", read memory, write memory, that sort of thing. This stuff is stored in object (.o) files.

    Now, a long time ago, computer scientists invented this "subroutine" thing. Execute-this-chunk-of-code-and-return-here. It wasn't too long before they realised that the most useful subroutines could be stored in a special place and used by any program that needed them.

    Now in the early days programmers would have to punch in the memory address that these subroutines were located at. Something like CALL 0x5A62. This was tedious and problematic should those memory addresses ever need to be changed.

    So, the process was automated. You write a program that calls printf(), and the compiler doesn't know the memory address of printf. So the compiler just writes CALL 0x0000, and adds a note to the object file saying "must replace this 0x0000 with the memory location of printf".

    Static linkage means that the linker program (the GNU one is called ld) adds printf's machine code directly to your executable file, and changes the 0x0000 to the address of printf. This happens when your executable is created.

    Dynamic linkage means that the above step doesn't happen. The executable file still has a note that says "must replace 0x000 with the memory location of printf". The operating system's loader needs to find the printf code, load it into memory, and correct the CALL address, each time the program is run.

    It's common for programs to call some functions which will be statically linked (standard library functions like printf are usually statically linked) and other functions which are dynamically linked. The static ones "become part" of the executable and the dynamic ones "join in" when the executable is run.

    STATIC LINKING:
    - Static linking is the result of the linker copying all library routines used in the program into the executable image.
    - Require more disk space and memory than dynamic linking,
    - but is both faster and more portable, since it does not require the presence of the library on the system where it is run.
    - When you statically link a file into an executable, the contents of that file are included at link time.
    - In other words, the contents of the file are physically inserted into the executable that you will run.

    DYNAMIC LINKING:
    - Dynamic linking is accomplished by placing the name of a sharable library in the executable image.
    - Actual linking with the library routines does not occur until the image is run, when both the executable and the library are placed in memory.
    - An advantage of dynamic linking is that multiple programs can share a single copy of the library.

    - The distinction is made for, among other things, allowing third party libraries to be included in your executable without you seeing their source code (such as libraries for database access, network communications and graphical user interfaces), or for compiling code in different languages (C and assembly code for example) and then linking them all together.

    - When you link dynamically, a pointer to the file being linked in (the file name of the file, for example) is included in the executable and the contents of said file are not included at link time. It's only when you later run the executable that these dynamically linked files are bought in and they're only bought into the in-memory copy of the executable, not the one on disk.

    It's basically a method of deferred linking. There's an even more deferred method (called late binding on some systems) that won't bring in the dynamically linked file until you actually try to call a function within it.

27.
How to resolve Hash Collisions

    1. Chaining
       - Start creating a list
       - Deletion is easy

    Open Addressing:
       - Deletion is tricky
    2a. Linear Probing:
       - If element present, then traverse linearly to find next empty bucket

    2b. Double Hashing:
       - If element present, use a second hash function. 
       - Say first hash function gave (5) and element is present. Second hash function gave 8
         Next go to 13, 21, 29 etc to find the first free spot

28.
Multithreading Best Practices
stackoverflow.com/questions/660621/threading-best-practices

    Immutability is great for multi-threading. Functional programming works well concurrently partly due to the emphasis on immutability.
    Use locks when you access mutable shared data, both for reads and writes.
    Don't try to go lock-free unless you really have to. Locks are expensive, but rarely the bottleneck.
    Monitor.Wait should almost always be part of a condition loop, waiting for a condition to become true and waiting again if it's not.
    Try to avoid holding locks for longer than you need to.
    If you ever need to acquire two locks at once, document the ordering thoroughly and make sure you always use the same order.
    Document the thread-safety of your types. Most types don't need to be thread-safe, they just need to not be thread hostile (i.e. "you can use them from multiple threads, but it's your responsible for taking out locks if you want to share them)
    Don't access the UI (except in documented thread-safe ways) from a non-UI thread. In Windows Forms, use Control.Invoke/BeginInvoke

    So the first step is: replace the implementation with one that doesn't use multiple threads at all.

    Then carefully put threading back in if, and only if, you discover a genuine need for it, when you've figured out some very simple safe ways to do so. A non-threaded implementation that works reliably is far better than a broken threaded implementation.

    When you're ready to start, favour designs that use thread-safe queues to transfer work items between threads and take care to ensure that those work items are accessed only by one thread at a time.

    Try to avoid just spraying lock blocks around your code in the hope that it will become thread-safe. It doesn't work. Eventually, two code paths will acquire the same locks in a different order, and everything will grind to a halt (once every two weeks, on a customer's server). This is especially likely if you combine threads with firing events, and you hold the lock while you fire the event - the handler may take out another lock, and now you have a pair of locks held in a particular order. What if they're taken out in the opposite order in some other situation?

    In short, this is such a big and difficult subject that I think it is potentially misleading to give a few pointers in a short answer and say "Off you go!" - I'm sure that's not the intention of the many learned people giving answers here, but that is the impression many get from summarised advice.

28b. Wait vs Sleep
http://stackoverflow.com/questions/1036754/difference-between-wait-and-sleep

    A wait can be "woken up" by another thread calling notify on the monitor which is being waited on whereas a sleep cannot.
    Also a wait (and notify) must happen in a block synchronized on the monitor object whereas sleep does not:

        Object mon = ...;
        synchronized (mon) {
            mon.wait();
        } 

    At this point the currently executing thread waits and releases the monitor.
    Another thread may do

        synchronized (mon) { mon.notify(); }

    One key difference not yet mentioned is that while sleeping a Thread does not release the locks it holds, while waiting releases the lock on the object that wait() is called on.

        synchronized(LOCK) {
            Thread.sleep(1000); // LOCK is held
        }


        synchronized(LOCK) {
            LOCK.wait(); // LOCK is not held
        }

29.
What is Amdhal's law

    In computer programming, Amdahl's law is that, in a program with parallel processing , a relatively few instruction s that have to be performed in sequence will have a limiting factor on program speedup such that adding more processor s may not make the program run faster. 

30.
UCS-2 vs UTF-8/16/32
http://stackoverflow.com/questions/3473295/utf-8-or-utf-16-or-utf-32-or-ucs-2
http://lucumr.pocoo.org/2014/1/9/ucs-vs-utf8/

    UCS2 and UCS4 are 2 and 4 byte encodings for the Universal Character Set (UCS).

    Use UTF-8 when the assumption of 8-bit code units is important.

    This applies to:
        Filenames and related OS calls on Unix systems, which had an established tradition of allowing variable-width encodings, but can't accept '\x00 bytes within strings and thus can't use UTF-16 or UTF-32. In fact, UTF-8 was originally designed for a Unix-based OS (Plan 9).
        Communications protocols designed around streams of octets.
        Anything that requires binary compatibility with US-ASCII but gives no special treatment to byte values above 127.

    First of all, forget about UCS-2: it is obsolete.
    It contains only a subset of Unicode characters.
    Forget about UTF-32 too: it is very large and very redundant. It is not useful for data transmission.

    UCS-2 is obsolete and has since been replaced with UTF-16
    UCS-2 is a fixed width encoding scheme while UTF-16 is a variable width encoding scheme
    UTF-16 capable applications can read UCS-2 files but not the other way around
    UTF-16 supports right to let scripts while UCS-2 does not
    UTF-16 supports normalization while UCS-2 does not

    When the Unicode consortium gave up on UCS-2 because it was no longer possible to support all of Unicode as a fixed length encoding with just 16 bits per unit two things had to happen: they needed to introduce a variable length encoding with 16 bit unit sizes and they had to reduce the total number of code points that can ever be addressed to find a way to make UTF-16 get the same characteristics as UTF-8. Namely it was necessary that if the stream of data was corrupted, you would not be presented with potentially misleading characters. For instance if a character did not fit into a single 16 bit unit and needed a second one, it was necessary that everything continues working. If the first one was lost in transmission the second one is actually an invalid character that can be ignored and not accidentally taken for a wrong character.

    In order to achieve that they came up with the concept of surrogate pairs to encode characters if they don't fit into ~16 bit. Essentially UTF-16 is now defined as an encoding that is either 2 or 4 bytes long but never more than that. The way this works is that 65.536 characters fit into just one unit. If a character is outside of that range it gets split up into two units of 16 bits each and the data is mangled in a way that makes them uniquely distinct. If you would look at one of the two surrogates in the pair separately without the knowledge of the other, it would become apparently that this is in incomplete character. For this trick to work the range D800 to DBFF are reserved for the trail surrogates and DC00 to DF00 for the lead ones. As an end result the total range of what can be encoded by UTF-16 is the lowest of all UTF formats.

31.
When will you use ASCII instead of UTF-8?
http://softwareengineering.stackexchange.com/questions/97247/what-is-the-advantage-of-choosing-ascii-encoding-over-utf-8

    In some cases it can speed up access to individual characters.
    Imagine string str='ABC' encoded in UTF8 and in ASCII (and assuming that the language/compiler/database knows about encoding)

    To access third (C) character from this string using array-access operator which is featured in many programming languages you would do something like c = str[2].

    Now, if the string is ASCII encoded, all we need to do is to fetch third byte from the string.

    If, however string is UTF-8 encoded, we must first check if first character is a one or two byte char, then we need to perform same check on second character, and only then we can access the third character.
    The difference in performance will be the bigger, the longer the string.

32.
Number of characters in ASCII

    There are 256 ASCII characters. This includes standard ASCII characters(0-127) and Extended ASCII characters(128-255).

33.
Unicode and Character Set
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/

34.
How to read a file in reverse
http://stackoverflow.com/questions/8664705/how-to-read-file-from-end-to-start-in-reverse-order-in-java

    Apache Commons IO has the ReversedLinesFileReader

35. How to programmatically cause a core dump in C/C++
https://stackoverflow.com/questions/979141/how-to-programmatically-cause-a-core-dump-in-c-c
    Raising of signal number 6 (SIGABRT in Linux) is one way to do it (though keep in mind that SIGABRT is not required to be 6 in all POSIX implementations so you may want to use the SIGABRT value itself if this is anything other than quick'n'dirty debug code).

    #include <signal.h>
    : : :
    raise (SIGABRT);
    Calling abort() will also cause a core dump, and you can even do this without terminating your process by calling fork() followed by abort() in the child only

36. How to store a password in database
https://www.geeksforgeeks.org/store-password-database/
https://stackoverflow.com/questions/1054022/best-way-to-store-password-in-database

    Storing plain text passwords in the database is a sin.


    One might also think that if not plain text then we must encrypt the password and then store. It is also a terrible idea.   
        - Encryption functions provide one-one mapping between input and output and they are always reversible. If the hacker gets the key, he will be able to decrypt the passwords. 
        - The better way would be to use a one way cryptographic hash function. Hash function provides a many-one mapping between input and output and it is practically impossible to reverse a output. A good cryptographic hash function has lesser number of Collisions (i.e for different input values to the function it is difficult to get the same output). Collisions cannot be completely avoided because of pigeonhole principle. For hashing passwords we can assume that the hash function will generate unique output i.e for no two different passwords we will get a same hash value.

    Some of the popular cryptographic hash functions are MD5 and SHA1. Instead of storing plain text password in the database one way is to store the hash of the password. You might be thinking that if we cannot get the actual password back from the hash then how are we going to verify the credentials that the user entered? It¿s simple, apply the same hash function on the password which user entered and then compare it with the hash stored in the database. If both hashes match then the user is authenticated (since hash of same input will give same output). Now if the attacker is able to get database access, he will be only able to view the hashed output and not the actual password.

    Using cryptographic hash function is better than storing plain text password.

    Hackers are smart guys and once they came to know that developers are storing hashed passwords, they pre-computed hash of large number of words (from a popular word list or dictionary words). They created a table of words and their corresponding hashes. This table is known as Rainbow Table and it is readily available online. They can use this table to reverse lookup the actual password by comparing the hashes obtained from the database. Hence it is very important to have a strong password since the possibility of your password appearing in the word list becomes less.

    There is still something that developers can do to keep your passwords away from prying eyes of the hackers. Make the passwords delicious by adding some salt to them! Yeah, right..! Add a salt. A salt is random data that is concatenated with your password before sending it as the input of the hashing function.

    For example :
    If your password is abc and the salt is !ZaP0#8, the result of hashFunction(¿abc!ZaP0#8¿) will be stored in the database instead of hashFunction(¿abc¿).
    Hence the rainbow table attacks won¿t be effective now as the probability that rainbow table contains hash of ¿abc!ZaP0#8¿ is meager (because generally rainbow tables are constructed from common words, dictionary words etc). Salt is not stored in the database and only present in the application configuration file which is not accessible to outer world. Gaining access to the source files is difficult than gaining access to the database.

    The above salting method is static. We have one fixed salt for all the passwords. To authenticate the user, first concatenate the fixed salt to the user supplied input (password) and then pass the value to the hashing function and compare it with the value stored in the database. However this approach is still vulnerable to brute-force and if the attacker is able to get the static salt he can use the old attack methodology by concatenating the salt in every word.

    A better approach would be to use a dynamic salt. For each user a new salt is generated by cryptographically strong random string generator. The password entered by user is concatenated with a random generated salt as well as a static salt. The concatenated string is passed as the input of hashing function. The result obtained is stored in database. Dynamic salt is required to be stored in the database since it is different for different users. When the user is to be authenticated, first the value of dynamic salt for that user is fetched from the database, it is concatenated with user supplied input and the static salt. The result is compared with the hash stored in the database.

    If the database is compromised the hacker will not only get your password hashes but also the dynamic salt used. You might be wondering then what is the advantage of dynamic salt over static salt if attacker has dynamic salt? Even if the attacker has dynamic salt he needs to create a new hash-table (or rainbow table) for each and every user present in the database (as per dynamic salt). This is a lot more expensive operation than creating just one table for all the users.

37. Abstract Class vs Interface - When to use each
https://stackoverflow.com/questions/479142/when-to-use-an-interface-instead-of-an-abstract-class-and-vice-versa

Use abstract classes and inheritance if you can make the statement ¿A is a B¿. Use interfaces if you can make the statement ¿A is capable of [doing] as¿

When we talk about abstract classes we are defining characteristics of an object type; specifying what an object is.

When we talk about an interface and define capabilities that we promise to provide, we are talking about establishing a contract about what the object can do.


---------------------------------------------------------------------------------------------
UML / DESIGN Questions:
0. Associations
    B --------<>        A  - Aggregation - Car and Engine; A "has an" instance of B; B can exist without A; 
    B --------<> Shaded A  - Composition - Visitor Center and Lobby; A "has an" instance of B; B can not exist without A; 
    B ---------|>       A  - Extends     - Animal and Cat,Goat etc

    Class Diagram has
        - Name
        - Attributes
        - Methods

    Abstract Class is represented using <<Animal>> or Italic

     ---------------
    |Employee       |
     ---------------
    |- Name : String|
    |- Id : Int     |
     ---------------
    |+ updateName   |
     ---------------

    + : Public
    - : Private
    # : Protected
    ~ : Default

    Interace : A is capable of B; Animal is capable of Moving
    Abstract Class: A is a B; Cat is an Animal

1. Parking lot  
https://www.educative.io/courses/grokking-the-object-oriented-design-interview/gxM3gRxmr8Z
http://stackoverflow.com/questions/764933/amazon-interview-question-design-an-oo-parking-lot

2. Coffee Maker
http://www.drdobbs.com/object-oriented-analysis-and-design-part/184403494

3. Deck of Cards
http://latentcontent.net/2010/03/27/anatomy-of-an-interview-question-design-a-deck-of-cards/
http://programmers.stackexchange.com/questions/284113/i-need-a-data-structure-for-a-card-game

4. What is Cardinality
    https://www.youtube.com/watch?v=QpdhBUYk7Kk&t=344s
    --------|- One
    --------<- Many
    -------||- One and only one     - An Order can have only one customer
    ------0-|- Zero or One
    ------|-<- One or Many          - An Order should have one or many products
    ------0-<- Zero or Many         - A customer can have Zero or Many Order

    3a. In Data Modelling:
        - the relationship that one table can have with another table. These relationships include: many-to-many, many-to-one/one-to-many, or one-to-one

        - Suppose we have three tables that are used by a company to store employee information: an Employee table, an Employee_Salary table, and a Department table.
        - The Department table will have a one to many relationship with the Employee table, because every employee can belong to only one department,
          but a department can consist of many employees.
        - In other words, the cardinality of the Department table in relationship to the employee table is one to many.
        - The cardinality of the Employee table in relationship to the Employee_Salary table will be one to one, since an employee can only have one salary, and vice versa

    3b. In SQL:
        - remember that the cardinality is a number.
          For example, let’s say we have a table with a “Sex” column which has only two possible values of “Male” and “Female”.
          Then, that “Sex” column would have a cardinality of 2

5. What are mock Objects
http://stackoverflow.com/questions/3622455/what-is-the-purpose-of-mock-objects
    - Fake objects to test intermediate functions.
      Eg: Cook <- Waiter <- Customer

6. UML Diagrams
    5.1 Use Case Diagram:
        - Has Actors, Scenarios and Use cases

        - It is used to get the overview of the system; 
          i.e The functionality of the system
        - It gives a basic understanding of WHO uses the SYSTEM and how the system is USED
        - It does not say anything about HOW the SYSTEM WILL BE DESIGNED

    5.2 CLASS DIAGRAM
        - This diagram identifies the key components of the system and how the system should be designed.
        - It represents the STATIC structure of the project.
        - It has
            - Class name
            - attributes
            - methods

    5.3 OBJECT DIAGRAM
        - It gives a REPRESENTATION of a cLASS diagram at any point of time.
        - I would say OBJECT DIAGRAM as an EXAMPLE of a CLASS DIAGRAM


    5.4 SEQUENCE DIAGRAM
        - Shows interaction between objects at any given time.
        - This helps in better understanding how the system should be implemented.
        - IT shows how message flows from One object to another.

        Async Messages: 
            Thin Overhead

        Sync MEssages:
            Dark Overhead

7. Factory Pattern vs Singleton Pattern
http://stackoverflow.com/questions/2094211/difference-between-singleton-and-factory-pattern
    A singleton pattern ensures that you always get back the same instance of whatever type you are retrieving, whereas the factory pattern generally gives you a different instance of each type

    The Singleton pattern ensures that only one instance of the class exists and typically provides a well-known, i.e., global point for accessing it.
    The purpose of the singleton is where you want all calls to go through the same instance.
    An example of this might be a class that manages a disk cache, or gets data from a static dictionary; wherever it is important only one known instance interacts with the resource.
    This does make it less scalable.

    The Factory pattern defines an interface for creating objects (no limitation on how many) and usually abstracts the control of which class to instantiate.
    The purpose of the factory is to create and return new instances.
    Often, these won't actually be the same type at all, but they will be implementations of the same base class.
    However, there may be many instances of each type

SINGLETON:
    restricting the instantiation of a class to one object

8. MVC PAttern
http://programmers.stackexchange.com/questions/127624/what-is-mvc-really

    The model manages fundamental behaviors and data of the application. It can respond to requests for information, respond to instructions to change the state of its information, and even to notify observers in event-driven systems when information changes. This could be a database, or any number of data structures or storage systems. In short, it is the data and data-management of the application.

    The view effectively provides the user interface element of the application. It'll render data from the model into a form that is suitable for the user interface.

    The controller receives user input and makes calls to model objects and the view to perform appropriate actions.

    MVC (Model, View, Controller) is a pattern for organising code in an application to improve maintainability.
    Imagine a photographer with his camera in a studio. A customer asks him to take a photo of a box.

    The box is the model, the photographer is the controller and the camera is the view.

    Because the box does not know about the camera or the photographer, it is completely independent. This separation allows the photographer to walk around the box and point the camera at any angle to get the shot/view that he wants.

    Non-MVC architectures tend to be tightly integrated together. If the box, the controller and the camera were one-and-the-same-object then, we would have to pull apart and then re-build both the box and the camera each time we wanted to get a new view. Also, taking the photo would always be like trying to take a selfie - and that's not always very easy.


9. Elevator
    https://stackoverflow.com/questions/19712592/aggregation-in-class-diagram-and-class-properties
    https://massivetechinterview.blogspot.com/2015/07/thought-works-object-oriented-design.html
    https://www.cs.cmu.edu/~luluo/Courses/18540PhDreport.pdf
    See Diagrams

10. Bike Shop

---------------------------------------------------------------------------------------------
OS NOTES:
-1.
Every program assumes it has its own memory space

-1b.
Memory allocation at compile time
http://stackoverflow.com/questions/21350478/what-does-memory-allocated-at-compile-time-really-mean/21350570#21350570
http://stackoverflow.com/questions/2445242/what-does-the-kernel-virtual-memory-of-each-process-contain
https://blogs.technet.microsoft.com/markrussinovich/2008/11/17/pushing-the-limits-of-windows-virtual-memory/

    VERY IMP:
    - Every program / process assumes that it has its own entire memory space (From 0x00000000 to 0xFFFFFFFF for example).
    - i.e. In a 32 bit machine, Every process assumes that it has 2^32 -1 bytes of memory to use (i.e. 4GB)
    https://msdn.microsoft.com/en-us/library/windows/desktop/aa366912(v=vs.85).aspx
        The following table shows the default memory range for each partition.
            Memory range	                            Usage
            Low 2GB (0x00000000 through 0x7FFFFFFF)	    Used by the process.
            High 2GB (0x80000000 through 0xFFFFFFFF)	Used by the system.

    - i.e. In a 64 bit machine, Every process assumes that it has 0x0000 0000 to 0xFFFF FFFF memory to use
    https://msdn.microsoft.com/en-us/library/windows/desktop/aa384271(v=vs.85).aspx

    For example, consider a global array:
        int array[100];

    The compiler knows at compile-time the size of the array and the size of an int, so it knows the entire size of the array at compile-time.
    Also a global variable has static storage duration by default:
    It is allocated in the static memory area of the process memory space (.data/.bss section).
    Given that information, the compiler decides during compilation in what address of that static memory area the array will be.

    Of course that memory addresses are virtual addresses.
    The program assumes that it has its own entire memory space (From 0x00000000 to 0xFFFFFFFF for example).
    That's why the compiler could do assumptions like "Okay, the array will be at address 0x00A33211".
    At runtime that addresses are translated to real/hardware addresses by the MMU and OS.

    - Memory allocated at compile time simply means there will be no further allocation at run time -- no calls to malloc, new, or other dynamic allocation methods.
    - You'll have a fixed amount of memory usage even if you don't need all of that memory all of the time.

    Q: Isn't memory allocation by definition a runtime concept ?
    A: The memory is not in use prior to run time, but immediately prior to execution starting its allocation is handled by the system.

    Q: If I make a 1KB statically allocated variable in my C/C++ code, will that increase the size of the executable by the same amount ?
    A: Simply declaring the static will not increase the size of your executable more than a few bytes.
       Declaring it with an initial value that is non-zero will (in order to hold that initial value).
       Rather, the linker simply adds this 1KB amount to the memory requirement that the system's loader creates for you immediately prior to execution.

-1c. When we associate '&' with a variable the address we get is the virtual address or the physical address? [duplicate]

    Processes on systems with virtual memory always deal with virtual addresses. You wouldn't be able to use a physical address from within a process.

    This is most easily verified by making the program run in a loop and print the value with some delay, then running multiple copies of the same program. Chances are they will print the same address (unless the OS is randomizing the virtual address usage), which of course would be impossible if the addresses were physical.

    Physical addresses occur in hardware, not software. A possible/occasional exception is in the operating system kernel. Physical means it's the address that the system bus and the RAM chips see.

    Not only are physical addresses useless to software, but it could be a security issue. Being able to access any physical memory without address translation, and knowing the addresses of other processes, would allow unfettered access to the machine.

-1d.
What determines the size of an executable
http://stackoverflow.com/questions/12920955/what-decides-the-size-of-my-executable

    Dynamically link to the C++ runtime.
    Compile the executable without debugging information.
    Compile with 'optimize for size' flags.
    Remove #iostream and fstream if you can, and use low level instead
    Also, using a lot of templates can cause size increase as well
    Use of lot of inline functions can cause increase in size.
    If you have lot of static and global variables then try to minimize them if possible.

0.
Differentiate Threads and Processes
http://stackoverflow.com/questions/1762418/process-vs-thread
IMP:
    THE TYPICAL DIFFERENCE IS THAT THREADS (OF THE SAME PROCESS) RUN IN A SHARED MEMORY SPACE, WHILE PROCESSES RUN IN SEPARATE MEMORY SPACES.

    - A process is an executing instance of an application.
    - Another difference between a thread and a process is that threads within the same process share the same address space, whereas different processes do not.
    - This allows threads to read from and write to the same data structures and variables, and also facilitates communication between threads.
    - Communication between processes – also known as IPC, or inter-process communication – is quite difficult and resource-intensive.

    Recently, I have been asked a question in an interview what's the difference between a process and a thread. Really, I did not know the answer. I thought for a minute and gave a very weird answer.

    Threads share the same memory.. processes do not. After answering this, the interviewer gave me an evil smile and fired the following questions at me:

    Q. Do you know the segments in which a program gets divided?
       My answer: yep (thought it was an easy one) Stack, Data, Code, Heap

    Q. So, tell me which segments share threads?
       You're pretty much correct, but THREADS SHARE ALL SEGMENTS EXCEPT THE STACK.
       Threads have independent call stacks, however the memory in other thread stacks is still accessible and in theory you could hold a pointer to memory in some other thread's local stack frame (though you probably should find a better place to put that memory!).

    - What does that mean? Well, for example, when you double-click the Microsoft Word icon, you start a process that runs Word.
    - A thread is a path of execution within a process.
    - Also, a process can contain multiple threads.
    - When you start Word, the operating system creates a process and begins executing the primary thread of that process. 

    Both processes and threads are independent sequences of execution.

    1. Threads share the address space of the process that created it; processes have their own address space.
    2. Threads have direct access to the data segment of its process; processes have their own copy of the data segment of the parent process.
    3. Threads can directly communicate with other threads of its process; processes must use interprocess communication to communicate with sibling processes.
    4. Threads have almost no overhead; processes have considerable overhead.
    5. New threads are easily created; new processes require duplication of the parent process.
    6. Threads can exercise considerable control over threads of the same process; processes can only exercise control over child processes.
    7. Changes to the main thread (cancellation, priority change, etc.) may affect the behavior of the other threads of the process; changes to the parent process does not affect child processes.

0b.
Explain Multithreading to a 5year old
http://programmers.stackexchange.com/questions/54359/how-would-you-explain-multi-threading-to-a-seven-year-old-kid/54519

    You have five jobs to do. You need to start working on all of them right now.
    Each job is a thread.
    You are the processor.
    Spend a little bit of time working on each job and then move to the next one, making sure you give attention to all of them.
    If you have more people, a job can only be worked on by one person at a time.
    Since each person can work on a different job, more people can get all the work done faster, if you have more than one job.

1.
What is a Race Condition
http://stackoverflow.com/questions/34510/what-is-a-race-condition

    A race condition is an undesirable situation that occurs when a device or system attempts to perform two or more operations at the same time, but because of the nature of the device or system, the operations must be done in the proper sequence to be done correctly.

    A race condition occurs when two or more threads can access shared data and they try to change it at the same time.
    Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data.
    Therefore, the result of the change in data is dependent on the thread scheduling algorithm, i.e. both threads are "racing" to access/change the data.

    Problems often occur when one thread does a "check-then-act"
    (e.g. "check" if the value is X, then "act" to do something that depends on the value being X) and another thread does something to the value in between the "check" and the "act". E.g:
        if (x == 5) // The "Check"
        {
           y = x * 2; // The "Act"

           // If another thread changed x in between "if (x == 5)" and "y = x * 2" above,
           // y will not be equal to 10.
        }

    Race conditions can be avoided by employing some sort of locking mechanism before the code that accesses the shared resource:

2. Mutex vs Semaphores:
    http://www.geeksforgeeks.org/mutex-vs-semaphore/
    https://blog.feabhas.com/2009/09/mutex-vs-semaphores-–-part-1-semaphores/

    Strictly speaking, a mutex is locking mechanism used to synchronize access to a resource.
    Only one task (can be a thread or process based on OS abstraction) can acquire the mutex.
    It means there will be ownership associated with mutex, and only the owner can release the lock (mutex).

    Semaphore is signaling mechanism (“I am done, you can carry on” kind of signal).
    For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend called you, an interrupt will be triggered upon which an interrupt service routine (ISR) will signal the call processing task to wakeup.

    Mutex can be released only by thread that had acquired it, while you can signal semaphore from any other thread (or process), so semaphores are more suitable for some synchronization problems like producer-consumer.

    So, if you have a number of instances of a resource (say three tape drives), you could use a semaphore with a count of 3. Note that this doesn't tell you which of those tape drives you have, just that you have a certain number.

    Also with semaphores, it's possible for a single locker to lock multiple instances of a resource, such as for a tape-to-tape copy. If you have one resource (say a memory location that you don't want to corrupt), a mutex is more suitable.

    Equivalent operations are:

    Counting semaphore          Mutual exclusion semaphore
    --------------------------  --------------------------
      Claim/decrease (P)                  Lock
      Release/increase (V)                Unlock

    Semaphore - Signal Entering and Leaving of a Critical Section

    OWNERSHIP:
        The mutex is similar to the principles of the binary semaphore with one significant difference: the principle of ownership.
        Ownership is the simple concept that when a task locks (acquires) a mutex only it can unlock (release)

3. Spin Locks vs Mutex:
http://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex
    The Theory
    In theory, when a thread tries to lock a mutex and it does not succeed, because the mutex is already locked, it will go to sleep, immediately allowing another thread to run. It will continue to sleep until being woken up, which will be the case once the mutex is being unlocked by whatever thread was holding the lock before. When a thread tries to lock a spinlock and it does not succeed, it will continuously re-try locking it, until it finally succeeds; thus it will not allow another thread to take its place (however, the operating system will forcefully switch to another thread, once the CPU runtime quantum of the current thread has been exceeded, of course).

    The Problem
    The problem with mutexes is that putting threads to sleep and waking them up again are both rather expensive operations, they'll need quite a lot of CPU instructions and thus also take some time. If now the mutex was only locked for a very short amount of time, the time spent in putting a thread to sleep and waking it up again might exceed the time the thread has actually slept by far and it might even exceed the time the thread would have wasted by constantly polling on a spinlock. On the other hand, polling on a spinlock will constantly waste CPU time and if the lock is held for a longer amount of time, this will waste a lot more CPU time and it would have been much better if the thread was sleeping instead.

    The Solution
    Using spinlocks on a single-core/single-CPU system makes usually no sense, since as long as the spinlock polling is blocking the only available CPU core, no other thread can run and since no other thread can run, the lock won't be unlocked either. IOW, a spinlock wastes only CPU time on those systems for no real benefit. If the thread was put to sleep instead, another thread could have ran at once, possibly unlocking the lock and then allowing the first thread to continue processing, once it woke up again.

    On a multi-core/multi-CPU systems, with plenty of locks that are held for a very short amount of time only, the time wasted for constantly putting threads to sleep and waking them up again might decrease runtime performance noticeably. When using spinlocks instead, threads get the chance to take advantage of their full runtime quantum (always only blocking for a very short time period, but then immediately continue their work), leading to much higher processing throughput.

    The Practice
    Since very often programmers cannot know in advance if mutexes or spinlocks will be better (e.g. because the number of CPU cores of the target architecture is unknown), nor can operating systems know if a certain piece of code has been optimized for single-core or multi-core environments, most systems don't strictly distinguish between mutexes and spinlocks. In fact, most modern operating systems have hybrid mutexes and hybrid spinlocks. What does that actually mean?

    A hybrid mutex behaves like a spinlock at first on a multi-core system. If a thread cannot lock the mutex, it won't be put to sleep immediately, since the mutex might get unlocked pretty soon, so instead the mutex will first behave exactly like a spinlock. Only if the lock has still not been obtained after a certain amount of time (or retries or any other measuring factor), the thread is really put to sleep. If the same code runs on a system with only a single core, the mutex will not spinlock, though, as, see above, that would not be beneficial.

    A hybrid spinlock behaves like a normal spinlock at first, but to avoid wasting too much CPU time, it may have a back-off strategy. It will usually not put the thread to sleep (since you don't want that to happen when using a spinlock), but it may decide to stop the thread (either immediately or after a certain amount of time) and allow another thread to run, thus increasing chances that the spinlock is unlocked (a pure thread switch is usually less expensive than one that involves putting a thread to sleep and waking it up again later on, though not by far).

    Summary
    If in doubt, use mutexes, they are usually the better choice and most modern systems will allow them to spinlock for a very short amount of time, if this seems beneficial. Using spinlocks can sometimes improve performance, but only under certain conditions and the fact that you are in doubt rather tells me, that you are not working on any project currently where a spinlock might be beneficial. You might consider using your own "lock object", that can either use a spinlock or a mutex internally (e.g. this behavior could be configurable when creating such an object), initially use mutexes everywhere and if you think that using a spinlock somewhere might really help, give it a try and compare the results (e.g. using a profiler), but be sure to test both cases, a single-core and a multi-core system before you jump to conclusions (and possibly different operating systems, if your code will be cross-platform).

3a
Mutex Monitor and Semaphore
http://www.programmerinterview.com/index.php/operating-systems/monitors-vs-semaphores/

    MONITOR:
    - A monitor is a set of multiple routines which are protected by a mutual exclusion lock.
    - None of the routines in the monitor can be executed by a thread until that thread acquires the lock.
    - This means that only ONE thread can execute within the monitor at a time.
    - Any other threads must wait for the thread that’s currently executing to give up control of the lock.

    - However, a thread can actually suspend itself inside a monitor and then wait for an event to occur.
    - If this happens, then another thread is given the opportunity to enter the monitor.
    - The thread that was suspended will eventually be notified that the event it was waiting for has now occurred, which means it can wake up and reacquire the lock.

    SEMAPHORE?
    - A semaphore is a simpler construct than a monitor because it’s just a lock that protects a shared resource – and not a set of routines like a monitor.
    - The application must acquire the lock before using that shared resource protected by a semaphore.

    - Use a Monitor unless you need to synchroize across process boundaries
    MONITOR:
        - Simpler to use than Semaphores
        - Handles all the details of lock acquisition and release
        - automatically acquire necessary locks
        - It is LIMITED to the Current Application Domain

        - Ensure thread safety which are IN PROCESS that is threads which are generated by an application (Internal Threads)

    SEMAPHORE:
        - Work with external threads
        - Ensure thread safety which are OUT PROCESS that is threads which are coming from outside of an application (External threads)

    MUTEX:
        - Synchronize threads across process
        - Can be shared across processes
        - Much more heavy-weight than a Monitor
        - Unlike monitors, however, a mutex can be used to synchronize threads across processes.

3b. 
Spin Lock in a Uniprocessor System
http://stackoverflow.com/questions/1025859/is-spin-lock-useful-in-a-single-processor-uni-core-architecture

    Spin locks must not be used on a single processor system.
    In the best case, a spin lock on a single processor system will waste resources, slowing down the owner of the lock; in the worst case, it will deadlock the processor.

    On single-processor systems, spinlocks are not needed because spinlock synchronization is required on high IRQLs only.
    On high IRQLs (above dispatch IRQL) a context switch cannot occur, so instead of spinning the acquiring thread can simply request an interrupt on the relevant IRQL and return; the interrupt will be masked until the releasing thread lowers the IRQL below the requested IRQL.

    For single processor systems, the kernel will ignore the spin count value, and treat it as zero - essentially making a spinlock a no-op.

    Yes, spin locks can be useful, and improve efficiency of some operations. However, generally you should start with a mutex, and if profiling show it to be a bottleneck, you may want to consider a spinlock. 

3c.
Lock Monitor Mutex and Semaphore
http://stackoverflow.com/questions/301160/what-are-the-differences-between-various-threading-synchronization-options-in-c?lq=1

    MONITOR / LOCK:
    Using a lock or monitor is useful for preventing the simultaneous execution of thread-sensitive blocks of code, BUT THESE CONSTRUCTS DO NOT ALLOW ONE THREAD TO COMMUNICATE AN EVENT TO ANOTHER.
    This requires synchronization events, which are objects that have one of two states, signaled and un-signaled, that can be used to activate and suspend threads.

    Mutex, Semaphores are OS-level concepts. e.g with a NAMED MUTEX you could synchronize across multiple (managed) exes (ensuring that only one instance of your application is running on the machine.)

    MUTEX:
    IMP:
    - Unlike monitors, however, a mutex can be used to synchronize threads across processes.
    When used for inter-process synchronization, a mutex is called a named mutex because it is to be used in another application, and therefore it cannot be shared by means of a global or static variable.
    It must be given a name so that both applications can access the same mutex object.
    In contrast, the Mutex class is a wrapper to a Win32 construct.
    While it is more powerful than a monitor, a mutex requires interop transitions that are more computationally expensive than those required by the Monitor class.

    SEMAPHORE:
    Use the Semaphore class to control access to a pool of resources.
    Threads enter the semaphore by calling the WaitOne method, which is inherited from the WaitHandle class, and release the semaphore by calling the Release method.
    The count on a semaphore is decremented each time a thread enters the semaphore, and incremented when a thread releases the semaphore.
    When the count is zero, subsequent requests block until other threads release the semaphore.
    When all threads have released the semaphore, the count is at the maximum value specified when the semaphore was created.
    A thread can enter the semaphore multiple times.
    The Semaphore class does not enforce thread identity on WaitOne or Release.
    programmers responsibility to not muck up.
    Semaphores are of two types: local semaphores and named system semaphores.
    If you create a Semaphore object using a constructor that accepts a name, it is associated with an operating-system semaphore of that name.
    Named system semaphores are visible throughout the operating system, and can be used to synchronize the activities of processes.
    A local semaphore exists only within your process.
    It can be used by any thread in your process that has a reference to the local Semaphore object.
    Each Semaphore object is a separate local semaphore.

3d.
Monitor vs Semaphore
http://stackoverflow.com/questions/7335950/semaphore-vs-monitors-whats-the-difference
    
    A Monitor is an object designed to be accessed from multiple threads.
    The member functions or methods of a monitor object will enforce mutual exclusion, so only one thread may be performing any action on the object at a given time.
    If one thread is currently executing a member function of the object then any other thread that tries to call a member function of that object will have to wait until the first has finished.

    A Semaphore is a lower-level object.
    You might well use a semaphore to implement a monitor.
    A semaphore essentially is just a counter.
    When the counter is positive, if a thread tries to acquire the semaphore then it is allowed, and the counter is decremented.
    When a thread is done then it releases the semaphore, and increments the counter.

http://www.programmerinterview.com/index.php/operating-systems/monitors-vs-semaphores/
    Both Monitors and Semaphores are used for the same purpose – thread synchronization.
    But, monitors are simpler to use than semaphores because they handle all of the details of lock acquisition and release.

    Another difference when using semaphores is that every routine accessing a shared resource has to explicitly acquire a a lock before using the resource.
    This can be easily forgotten when coding the routines dealing with multithreading . Monitors, unlike semaphores, automatically acquire the necessary locks.

    MUTEX:
        Mutex helps us to identify whether an application is acquired by an external thread or not and It allows only one single thread to enter to execute a particular task.
        It means mutex allows only one single external thread to enter and execute its task and same ensuring thread safety.
        IMP: a mutex can be used to synchronize threads across processes.

3e.
Monitor vs Mutex
http://www.albahari.com/threading/part2.aspx#_Mutex
http://www.onlinebuff.com/article_understand-monitor-vs-mutex-vs-semaphore-vs-semaphoreslim-onlinebuff_60.html

    Locks/Monitors ensures the thread safety which are in process that is threads which are generated by an application (Internal threads) it does not have any control over the threads which are coming from outside of an application.
    Locks/Monitors provides safety againts the threads generated by an application.

    Mutex ensures the thread safety which are out process that is threads which coming from outside of an application (External threads).
    Mutex provides safety againts the external threads.

    In a multiple instance of an application external threads are created so to ensure thread safety from an external threads we need to apply mutex.

http://stackoverflow.com/questions/1164038/monitor-vs-mutex-in-c-sharp
    A Monitor is managed, and more lightweight - but is restricted to your AppDomain.
    A Mutex can be named, and can span processes (allowing some simple IPC scenarios between applications), and can be used in code that wants a wait-handle).

    For most simple scenarios, Monitor (via lock) is fine.

    A Mutex can be shared across processes, and is much more heavy-weight than a Monitor.

    Use a Monitor unless you need to synchronize across process boundaries.

3f.
Semaphore vs Condition Variable
http://stackoverflow.com/questions/3513045/conditional-variable-vs-semaphore

    Conditional variable is essentially a wait-queue, that supports blocking-wait and wakeup operations, i.e. you can put a thread into the wait-queue and set its state to BLOCK, and get a thread out from it and set its state to READY.

    Note that to use a conditional variable, two other elements are needed:

        a condition (typically implemented by checking a flag or a counter)
        a mutex that protects the condition

    The protocol then becomes,

        acquire mutex
        check condition
        block and release mutex if condition is true, else release mutex

    SEMAPHORE IS ESSENTIALLY A COUNTER + A MUTEX + A WAIT QUEUE. And it can be used as it is without external dependencies. You can use it either as a mutex or as a conditional variable.

    Therefore, semaphore can be treated as a more sophisticated structure than conditional variable, while the latter is more lightweight and flexible.

    If all of your threads are waiting for some event, e.g., submission of a task, then you can wake them all up by using a condition variable upon an event.

    If you have a limited resource, say 10 pages of memory reserved for your threads, then you will need them to wait until a page is available. When this happens, you will need to let just one thread start execution. In this case you can use a semaphore unlock up as many threads as available pages.

4.
Multi-threaded programming in C++
http://stackoverflow.com/questions/266168/simple-example-of-threading-in-c
    Create a function that you want the thread to execute. I'll demonstrate with a trivial example:
        void task1(std::string msg)
        {
            std::cout << "task1 says: " << msg;
        }

    Now create the thread object that will ultimately invoke the function above like so:
        std::thread t1(task1, "Hello");

    (You need to #include <thread> to access the std::thread class)
    As you can see, the constructor's arguments are the function the thread will execute, followed by the function's parameters.

    Finally, join it to your main thread of execution like so:
        t1.join(); 

    (Joining means that the thread who invoked the new thread will wait for the new thread to finish execution, before it will continue it's own execution).
    The Code
        #include <string>
        #include <iostream>
        #include <thread>

        using namespace std;

        // The function we want to execute on the new thread.
        void task1(string msg)
        {
            cout << "task1 says: " << msg;
        }

        int main()
        {
            // Constructs the new thread and runs it. Does not block execution.
            thread t1(task1, "Hello");

           // Makes the main thread wait for the new thread to finish execution, therefore blocks its own execution.
            t1.join();
        }

4b.
Thread Join vs Detach
    If a thread is detached then the Main thread does not wait for the thread to finish its execution

5.
Hard Link vs Soft Link
    http://stackoverflow.com/questions/185899/what-is-the-difference-between-a-symbolic-link-and-a-hard-link

    Underneath the file system files are represented by inodes (or is it multiple inodes not sure)

    A file in the file system is basically a link to an inode.
    A hard link then just creates another file with a link to the same underlying inode.

    When you delete a file it removes one link to the underlying inode. The inode is only deleted (or deletable/over-writable) when all links to the inode have been deleted.

    A symbolic link is a link to another name in the file system.

    IMP:
    - Once a hard link has been made the link is to the inode.
    - deleting renaming or moving the original file will not affect the hard link as it links to the underlying inode.
    - Any changes to the data on the inode is reflected in all files that refer to that inode.

    NOTE:
    Hard links are only valid within the same File System.
    Symbolic links can span file systems as they are simply the name of another file.

    Create two files:
        $ touch blah1; touch blah2

    Enter some Data into them:
        $ echo "Cat" > blah1
        $ echo "Dog" > blah2

    And as expected:
        $cat blah1; cat blah2
        Cat
        Dog

    Let's create hard and soft links:
        $ ln blah1 blah1-hard
        $ ln -s blah2 blah2-soft

    Let's see what just happened:
        $ ls -l

        blah1
        blah1-hard
        blah2
        blah2-soft -> blah2

    Changing the name of blah1 does not matter:
        $ mv blah1 blah1-new
        $ cat blah1-hard
        Cat

    blah1-hard points to the inode, the contents, of the file - that wasn't changed.
        $ mv blah2 blah2-new
        $ ls blah2-soft
        blah2-soft
        $ cat blah2-soft  
        cat: blah2-soft: No such file or directory

    The contents of the file could not be found because the soft link points to the name, that was changed, and not to the contents.
    Likewise, If blah1 is deleted, blah1-hard still holds the contents; if blah2 is deleted, blah2-soft is just a link to a non-existing file.

5b.
What is an Inode ?
    INDEX NODE
    It is the metadata about a file - permissions, ownerships, timestamps
    It is a data structure that holds all relevant information about a file.

    The contents of the file are stored in a collection of data blocks

    File -> iNode -> Actual Data
    Inodes link to the physical data

    IMP: iNodes track broken up pieces of a file.
    iNode can tell ALL the blocks  that contain the data

    iNode contains indirect referencing if the FILE is too big

    inode is a Unix thing

    Microsoft uses - FILE RECORD ATTRIBUTES
    Apple use - Catalog FILES

A file system can run out of space in two ways :
    1. No space for adding new data is left
    2. All the Inodes are consumed.

    Well, the first way is pretty obvious but we need to look at the second way.
    Yes, its possible that a case arises where we have free storage space but still we cannot add any new data in file system because all the Inodes are consumed.
    This may happen in a case where file system contains very large number of very small sized files.
    This will consume all the Inodes and though there would be free space from a Hard-disk-drive point of view but from file system point of view no Inode available to store any new file.

Super block:
    All iNodes are stored in the iNode table which is the Superblock

Why doesn't iNode have the File name
    Multiple file names can map to the same file.
    File name is stored in the respective Directory.

Filesytem Basics
    - In unix everything is a file
      Even non-file devices such as terminals, printers, and disks themselves are abstracted and accessed via names in the file system. 

How "ls" command works:
http://sysadvent.blogspot.com/2010/12/day-15-down-ls-rabbit-hole.html
    1. Find by using strace
    - Find if "ls" can be found at $PATH
    - Bash spawns a child process to execute the "ls" program

5c. How is data stored? Direct referencing, Double indirect and triple indirect
https://www.sans.org/blog/understanding-indirect-blocks-in-unix-file-systems/

    There are pointers in the inode that link to the data blocks.
    The contents of the file are stored in a collection of data blocks

    There are only fifteen block pointers in the inode. Assuming standard 4K data blocks, that means that the largest possible file that could be addressed directly would be 60K

    Thirteenth pointer is the indirect block pointer. Once the file grows beyond 48K, the file system grabs a data block and starts using it to store additional block pointers, setting the thirteenth block pointer in the inode to the address of this block. Block pointers are 4-byte quantities, so the indirect block can store 1024 of them. That means that the total file size that can be addressed via the indirect block is 4MB (plus the 48K of storage addressed by the direct blocks in the inode).
    Once the file size grows beyond 4MB + 48KB, the file system starts using doubly indirect blocks. The fourteenth block pointer points to a data block that contains the addresses of other indirect blocks, which in turn contain the addresses of the actual data blocks that make up the file's contents. That means we have up to 1024 indirect blocks that in turn point to up to 1024 data blocks- in other words up to 1M total 4K blocks, or up to 4GB of storage.
    At this point, you've probably figured out that the fifteenth inode pointer is the trebly indirect block pointer. With three levels of indirect blocks, you can address up to 4TB (+4GB from the doubly indirect pointer, +4M from the indirect block pointer, +48K from the direct block pointers) for a single file.

6.
Multi Processor vs Multi Core System
http://superuser.com/questions/13107/whats-the-difference-between-multicore-proc-and-multiproc-system
http://theydiffer.com/difference-between-multicore-and-multiprocessor-systems/

    A multicore uses a single CPU while a multiprocessor uses multiple CPU¿s.

    Multiple processors let your computer do literally two things at once (instead of only seemingly doing two things at once, but actually just swapping between tasks extremely rapidly).

    Multiple cores are the same.
    The advantage of multiple cores over multiple processors is that they share some bits of the CPU, e.g. the second level cache, which makes it possible for them to work even more efficiently if they have some shared data.
    This makes them much cheaper to manufacture.
    A single dual-core CPU also takes up less room than two single-core CPUs, which is an important factor these days with everyone moving to laptops.

    A multiprocessor system contains more than one such CPU, allowing them to work in parallel.
    This is called SMP, or Simultaneous Multiprocessing.

    A multi*core* CPU has multiple execution cores on one CPU.
    Now, this can mean different things depending on the exact architecture, but it basically means that a certain subset of the CPU's components is duplicated, so that multiple "cores" can work in parallel on separate operations.
    This is called CMP, Chip-level Multiprocessing.

7.
What is a File descriptor
http://stackoverflow.com/questions/5256599/what-are-file-descriptors-explained-in-simple-terms

    In simple words, when you open a file, the operating system creates an entry to represent that file and store the information about that opened file.
    So if there are 100 files opened in your OS then there will be 100 entries in OS (somewhere in kernel).
    These entries are represented by integers like (...100, 101, 102....).
    This entry number is the file descriptor.
    So it is just an integer number that uniquely represents an opened file in operating system.
    If your process opens 10 files then your Process table will have 10 entries for file descriptors.

    Similarly when you open a network socket, it is also represented by an integer and it is called Socket Descriptor.

8.
Why can't RAM be replaced by Cache memory
https://www.quora.com/Why-cant-RAM-be-replaced-by-Cache-memory

    It's all about the memory hierarchy.
    In general, it's very difficult to make a semiconductor memory which has both a large capacity and a high speed.
    The former (capacity) requires a large physical size, while the latter (high speed) requires high power consumption as well as a high degree of locality (if your memory is 1 cm from your logic devices, it will be much slower than a memory that is 10 microns from the logic devices).

    Part of it comes down to cost.
    CPU cache memory is extremely expensive to manufacture in comparison to RAM, which is extremely expensive to manufacture in comparison to storage media like HDDs/SSDs.
    Then there's the logistics of fitting all that memory on the die of the CPU.
    Part of the reason why CPU cache memory is so fast is because it's directly accessible by the CPU, and doesn't have to be bussed, unlike RAM or HDD/SSD memory.
    Increasing cache size also proportionally increases latency, which defeats the advantage of the cache.

9.
How to avoid deadlocks
http://stackoverflow.com/questions/1057054/how-to-avoid-deadlocks
https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/7_Deadlocks.html

There are four conditions that are necessary to achieve deadlock:
    - Mutual Exclusion - At least one resource must be held in a non-sharable mode; If any other process requests this resource, then that process must wait for the resource to be released.
    - Hold and Wait - A process must be simultaneously holding at least one resource and waiting for at least one resource that is currently being held by some other process.
    - No preemption - Once a process is holding a resource ( i.e. once its request has been granted ), then that resource cannot be taken away from that process until the process voluntarily releases it.
    - Circular Wait - A set of processes { P0, P1, P2, . . ., PN } must exist such that every P[ i ] is waiting for P[ ( i + 1 ) % ( N + 1 ) ]. ( Note that this condition implies the hold-and-wait condition, but it is easier to deal with the conditions if the four are considered separately. )

    Deadlocks can be prevented by preventing at least one of the four required conditions:
    - Mutual Exclusion
        Shared resources such as read-only files do not lead to deadlocks.
    Unfortunately some resources, such as printers and tape drives, require exclusive access by a single process.

- Deadlock prevention or avoidance - Do not allow the system to get into a deadlocked state.
- Deadlock detection and recovery - Abort a process or preempt some resources when deadlocks are detected.

10.
Latency vs Throughput
https://community.cadence.com/cadence_blogs_8/b/sd/archive/2010/09/13/understanding-latency-vs-throughput

    Latency is the time required to perform some action or to produce some result.
    Latency is measured in units of time -- hours, minutes, seconds, nanoseconds or clock periods.

    Throughput is the number of such actions executed or results produced per unit of time.
    This is measured in units of whatever is being produced (cars, motorcycles, I/O samples, memory words, iterations) per unit of time.

    An assembly line is manufacturing cars.
    It takes eight hours to manufacture a car and that the factory produces one hundred and twenty cars per day.

        The latency is: 8 hours.
        The throughput is: 120 cars / day or 5 cars / hour.

11.
What is Page Fault

    A page fault (sometimes called #PF, PF or hard fault [a]) is a type of interrupt, called trap, raised by computer hardware when a running program accesses a memory page that is mapped into the virtual address space, but not actually loaded into main memory.

12.
Why do we need Paging
Unequal Size Fixed Partitioning and Dynamic Partitioning

    Fixed Partitioning:
        - Initial approach is to divide the main memory into FIXED SIZE chunk and allocate
          them to processes
        - PROBLEM: Wasted space in each parition, a.k.a INTERNAL FRAGMENTATION
        - Solution: Minimize it by using Varying Size of Fixed paritions
          So we can allocate a process to the FIRST largest block that is just large enough
          to fit it

    Dynamic Paritioning:
        - Divide Physical Memory into Chunks exactly the size you want.
        - So after a while, the physical memory will be completely fragmented
        - There could be BITs of space left but still not able to fit a full process
        - Thus resulting in EXTERNAL FRAGMENTATION

    Paging:
        - Process is split into Pages
        Pages of process memory may be displaced from the RAM to the disk.
        This is called swapping or paging.
        The data is moved to the swap space, and loaded back from the swap space when it is needed

        - Small blocks of EQUAL / FIXED sized partitions
        - Also divide the process into the same size
        - Internal Fragmentation same as Fixed Partitioning
        - INVISIBLE to programmer

        - Within a program, each logical address consists of a page number and an
          offset within the page

        VERY IMP:
        - OS maintains a PAGE TABLE for each Process
        - Each Logical Address within a program consists of a PAGE NUMBER and AN OFFSET within
          the page

    Segmentation:
        - Divide a user program into Segments of VARIABLE SIZE
        - External Fragmentation same as Dynamic Partitioning
        - VISIBLE TO programmer

12b.
If we have infinite memory, then do we still be needing paging
http://stackoverflow.com/questions/14214286/if-we-have-infinite-memory-then-do-we-still-be-needing-paging

    QUESTION:
    Paging creates illusion that each process has infinite RAM by moving pages to and from disk.
    So if we have infinite memory(in some hypothetical situation), do we still need Paging?

    SOLUTION:
    Although paging is often associated with the ability to swap pages in and out of RAM to a hard disk to conserve memory, this is merely one aspect of paging.

    Here are some other reasons to have paging:
    1. Security:
        Paging is a method to enforce operating system security and memory protection by ensuring that a processes cannot access the memory of another process and that it cannot modify the resident kernel.

    2. Multitasking:
        Paging aids in multitasking by virtualizing the memory space, that is, address 0xFOO in Process A can be something completely different than 0xFOO in Process B

    3. Memory Allocation:
        Paging aids in memory allocation by reducing fragmentation and ensuring RAM is only allocated when accessed.
        What this means is that although a process needs, suppose, 100MB of continuous RAM space, this need not be continuous physically.
        Additionally, when a program requests 100MB of space, the operating system will tell the program it's safe to use that 100MB of space, yet it will not be actually allocated until the program uses that space to its fullest.

13.
What is Virtual Memory
http://superuser.com/questions/42854/what-is-virtual-memory

    Virtual memory is a layer of abstraction provided to each process.
    The computer has, say, 2GB of physical RAM, addressed from 0 to 2G.
    A process might see an address space of 4GB, which it has entirely to itself.
    The mapping from virtual addresses to physical addresses is handled by a memory management unit, which is managed by the operating system.
    Typically this is done in 4KB "pages".

    This gives several features:
    - A process can not see memory in other processes (unless the OS wants it to!)
    - Memory at a given virtual address may not be located at the same physical address
    - Memory at a virtual address can be "paged out" to disk, and then "paged in" when it is accessed again.

http://stackoverflow.com/questions/9414565/understanding-virtual-address-and-virtual-address-space
    - Modern architectures let multiple programs execute as if they own the entire logical address space.
    - i.e. Several programs could write to memory location at the same address without stepping over each others' results.
    - This is done by virtualizing the address space:

    Let's say programs A and B generate a write to memory location at 0x1000.
    - The CPU, aided by the operating system, could performs additional adjustments to the address, and map it to physical address 0x60001000 for program A, and to 0x5F001000 for program B.
    - Both programs think that they wrote to the location at 0x1000, because they operate in a virtual address space.
    - Their model of the memory is a contiguous block starting at 0 and continuing to 0x000100000000 (assuming that your system has 4GiB of memory available to processes).
    - But this model works only because the CPU additionally translates their logical addresses to physical addresses, which are allocated and taken away as needed in the process of running the program.

    - Because the same number representing an address means different things to a program and to a CPU, the address space of the program is called virtual, and the address space of the CPU is called physical.

14.
Why do we need Virtual Memory
http://stackoverflow.com/questions/19349572/why-do-we-need-virtual-memory
http://computer.howstuffworks.com/question684.htm

    For example, if you load the Windows operating system, an e-mail program, a Web browser and word processor into RAM simultaneously, 64 megabytes is not enough to hold it all.
    If there were no such thing as virtual memory, your computer would have to say, "Sorry, you cannot load any more applications. Please close an application to load a new one."
    With virtual memory, the computer can look for areas of RAM that have not been used recently and copy them onto the hard disk.
    This frees up space in RAM to load the new application.
    Because it does this automatically, you don't even know it is happening, and it makes your computer feel like is has unlimited RAM space even though it has only 32 megabytes installed.
    Because hard-disk space is so much cheaper than RAM chips, virtual memory also provides a nice economic benefit.

15.
Where is Page File / Virtual Memory Stored
http://computer.howstuffworks.com/question684.htm

    The area of the hard disk that stores the RAM image is called a page file.
    It holds pages of RAM on the hard disk, and the operating system moves data back and forth between the page file and RAM.
    (On a Windows machine, page files have a .SWP extension.)

    The operating system has to constantly swap information back and forth between RAM and the hard disk. This is called THRASHING

    In Windows search for pagefile.sys. It should be in C:
    The size of page file can be configured

16.
What is Resident Set Size (RSS) and Virtual Memory Size (VSZ)
http://stackoverflow.com/questions/7880784/what-is-rss-and-vsz-in-linux-memory-management

    Resident set size (RSS) is the portion of memory occupied by a process that is held in main memory (RAM).
    The rest of the occupied memory exists in the swap space or file system, either because some parts of the occupied memory were paged out, or because some parts of the executable were never loaded
    - It does include memory from shared libraries as long as the pages from those libraries are actually in memory.
    - It does include all stack and heap memory.

    - VSZ is the Virtual Memory Size.
    - It includes all memory that the process can access, including memory that is swapped out and memory that is from shared libraries.

    So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped), and has only actually loaded 1000K of the shared libraries and 400K of its own binary then:
        RSS: 400K + 1000K + 100K = 1500K
        VSZ: 500K + 2500K + 200K = 3200K

17. Paging vs Swapping vs Trashing
    Paging is when individual memory segments, or pages, are moved to or from the swap area. When memory is low portions of a process (data areas, but not instructions which are available from local or remote file systems) are moved to free up memory space. Segments are chosen to be moved if they haven't been referenced recently. When the process next tries to reference this segment a page fault occurs and the process is suspended until the segment is returned to memory. A page fault is normally returned the first time a program is started, as it won't be in memory. It's then paged from the local or remote file system.

    Swapping happens under a heavier work load. With swapping the kernel moves all segments belonging to a process to the swap area. The process is chosen if it's not expected to be run for a while. Before the process can run again it must be copied back into physical memory.

    Thrashing It is a state in which our CPU perform 'productive' work less and 'swapping' more. CPU is busy in swapping pages, so much that it can not respond to user program as much as required. Why it occurs In our system Thrashing occurs when there are too much pages in our memory, and each page referes t an other page. The real memory shortens in capacity to have all the pages in it, so it uses 'virtual memory'. When each page in execution demands that page that is not currently in real memory (RAM) it place some pages on virtual memory and adjust the required page on RAM. If CPU is so much busy in doing this task, thrashing occurs

    In operating systems that implement a virtual memory space the programs allocate memory from an address space that may be much larger than the actual amount of RAM the system possesses. The OS is responsible for deciding which programs "memory" is in actual RAM. It needs a place to keep things while they are "out". This is what is called "swap space", as the OS is swapping things in and out as needed. When this swapping activity is occurring such that it is the major consumer of the CPU time, then you are effectively thrashing. You prevent it by running fewer programs, writing programs that use memory more efficiently, adding RAM to the system, or maybe even by increasing the swap size.

18. Why does a 32-bit OS support 4 GB of RAM?
https://stackoverflow.com/questions/1119278/why-does-a-32-bit-os-support-4-gb-of-ram
    Because 32 bits are able to represent numbers up to 232 ¿ 1 = 4294967295 = 4 GiB ¿ 1 and therefore address up to 232 individual bytes which would be 4 GiB then.

    Everybody is saying 2^32 = 4GiB, which is right. Just in case, here is how we got there:

    A 32-bit machine uses 32 bits to address memory. Each bit has a value of 0 or 1. If you have 1 bit, you have two possible addresses: 0 or 1. A two-bit system ( pun aside ) has four possible address: 00 =0, 01=1, 10=2, 11=3. 2^2=4. Three bits have 8 possble addresses: 000=0, 001=1, 010=2, 011=3, 100=4, 101=5, 110=6, and 111=7.

    Each bit doubles the potential address space, which is why 2^n tells you how many addresses you use for a given number of bits. 2^1 = 2, 2^2 = 2*2 = 4, 2^3 = 2*2*2 = 8, etc.

    By the time you get to 32 bits, you are at 4GiB.

19. MB vs MiB / GB vs GiB
    Gibibyte (GiB) is one of the standard units used in the field of data processing and data transmission (along with mebibyte (MiB), kibibyte (KiB), etc.). Gibibyte, mebibyte, and kibibyte are defined as powers of 2. 1 GiB equals 2 bytes or 1,073,741,824 bytes. Gibibyte is closely related to gigabyte (GB), which can be a synonym of gibibyte or can refer to 10 bytes or 1,000,000,000 bytes in accordance with International System of Units (SI).

20. SRAM vs DRAM
    Static Random Access Memory
    Dynamic Random Access Memory

    SRAM: 
        - retains data bits in its memory as long as power is being supplied
        - does not need to be periodically refreshed
        - faster
        - more expensive than DRAM
        - used in CPU cache

    DRAM:
        - stores each bit of data in a separate capacitor within an integrated circuit
        - The capacitor can be either charged or discharged; these two states are taken to represent the two values of a bit, conventionally called 0 and 1. 
        - A DRAM storage cell is dynamic in that it needs to be refreshed or given a new electronic charge every few milliseconds

21. Spurious Wakeups
http://tutorials.jenkov.com/java-concurrency/thread-signaling.html#spurious-wakeups
    For inexplicable reasons it is possible for threads to wake up even if notify() and notifyAll() has not been called. This is known as spurious wakeups. Wakeups without any reason.

    To guard against spurious wakeups the signal member variable is checked inside a while loop instead of inside an if-statement. Such a while loop is also called a spin lock. The thread awakened spins around until the condition in the spin lock (while loop) becomes false.

    public class MyWaitNotify3{

      MonitorObject myMonitorObject = new MonitorObject();
      boolean wasSignalled = false;

      public void doWait(){
        synchronized(myMonitorObject){
          while(!wasSignalled){
            try{
              myMonitorObject.wait();
             } catch(InterruptedException e){...}
          }
          //clear signal and continue running.
          wasSignalled = false;
        }
      }

  public void doNotify(){
    synchronized(myMonitorObject){
      wasSignalled = true;
      myMonitorObject.notify();
    }
  }
}

22. Memory Layout of a Program
https://cs-fundamentals.com/c-programming/memory-layout-of-c-program-code-data-segments
https://www.geeksforgeeks.org/memory-layout-of-c-program/

    Code / Text Segment - Read Only
    Data Segment - Non Read Only

    Code/Text Segment: 
        Code segment, also known as text segment contains machine code of the compiled program. The text segment of an executable object file is often read-only segment that prevents a program from being accidentally modified.

    Data Segment:
        Data segment stores program data. This data could be in form of initialized or uninitialized variables, and it could be local or global. Data segment is further divided into four sub-data segments (initialized data segment, uninitialized or .bss data segment, stack, and heap) 

        Initialized Data or Data Segment
            Initialized data or simply data segment stores all global, static, constant, and external variables (declared with extern keyword) that are initialized beforehand.

        Uninitialized Data or .bss Segment
            Data in this segment is initialized by the kernel to arithmetic 0 before the program starts executing uninitialized data starts at the end of the data segment and contains all global variables and static variables that are initialized to zero or do not have explicit initialization in source code.
---------------------------------------------------------------------------------------------
NETWORKS:
1. What is SSL Termination
2. Asymmetric and Symmetric Encryption
0. 
How TCP Works:
    7 Logical Layers
        Application
        Presentation
        Session
        Transport
        Network
        Data Link
        Physical

OSI Model:

    1. Application
        - Application that the user is interacting with
        - Eg: Firefox, Outlook, Chrome, Skype
        - Users are on this layer

        PROBLEMs:
        - Firefox could be corrputed
        - Misconfiguration of the application

    2. Presentation
        - Layer that the operating system is on
        - Lot of configurable information

        PROBLEMs:
        - Something is the OS is not allowing the users to go to internet
        - Could be a device driver problem where Wifi is not working

    3. Session
        - Layer that deals creating a Session between two computers
        - So if we are going to a website, our computer should create a session with the
          web server from which we want the information
        - Everytime a computer has to talk to another computer, a session should be 
          created between the two computers
    
        PROBLEMs:
        - Someone might have prohibited you from accessing the website

    4. Transport
        - Decides how much information should be sent at one time
        - Sliding window / Windowing happens here
        - How much information the server will send back
        - Deals with transfer of data back and forth
        - How long the computer will wait before the ACK is received

        PROBLEMs:

    5. Network
        - IP address are at this layer
        - TCP / IP falls here
        - Default gateways, Subnet Mask, DNS falls here
        - Routers operate on this layer
        - Choosing the best path to transfer information from one point to another

        PROBLEMs:

    6. Data Link:
        - Data would come all the way till your AP
        - Deciding on which computer or machine to deliver the packets will be decided
          by looking up the MAC address
        - MAC address reside here
        - Swithces operate on this layer
        - ARP falls here (Address Resolution Protocol)
        - All computers on a network are plugged into a switch so that they can talk to each
          other.
        - Those things happen here

        WIFI Access Point
http://superuser.com/questions/902831/which-layer-of-network-stack-does-access-point-operate
        - 802.11 is said to operate at layer 2
        - Access points belong to the data-link layer (layer 2) devices.

        - An access point cannot create a new network.
        - It only broadcasts whatever packet it receives...just like switches. A   r is a good example of a device that can create a new network

        PROBLEMs:

    7. Physical Layer:
        - Hubs, cabling, and repeaters
        - Where actual wiring happens
        - All the physical stuffs that connect computers

        PROBLEMs:
        - If the cable gets unplugged

1.
How TCP/IP Network works.
http://www.hardwaresecrets.com/how-tcp-ip-protocol-works-part-1/
Nice Link: http://www.hardwaresecrets.com/how-tcp-ip-protocol-works-part-1/

    - A network can be classified as TCP/IP and Ethernet at the same time.
      TCP/IP is a set of protocols that deals with layers 3 to 7 from the OSI reference model.
      Ethernet is a set of protocols that deals with layers 1 and 2 from the OSI reference model

                    Data
                      |
                      |
           TCP/UDP  Data
           Header 
                |
                |
    IP     TCP/UDP  Data
  Header   Header 
                |
                |

    1. Application Layer:
        - Application parses the request and sends to the Transport Layer.
        - Communication between programs and transport protocols
        - This layer handles,
            a. HTTP, FTP, SMTP

               Email    Browser     FTP Program
                 |         |            |
                SMTP      HTTP         FTP
                 |         |            |
              Port 25   Port 80      Port 20/21
                 |         |            |
              ------------TCP Layer-------------

        - APPLICATION LAYER TALKS TO TRANSPORT LAYER THROUGH PORT NUMBER.
        - Eg HTTP

    2. Transport Layer / Network Layer:
        - TCP and UDP comes here.

        - Divides data into packets
        - Puts back the packet in orders
        - Sending ACK back to the sender / receiver
        - Header will include information like,
            a. Socket: Port Number (Source and destination)
            b. Sequence Number ( for ordering)
            c. Checksum (if data is intact)

    3. Internet Layer:
        - IP is an unreliable protocol. NO ACK system.
        - Several hops to reach the destination
        - Adds Source and Target IP address
        - Responsible for routing of packets.
            Example: tracert www.google.com
        - This also deals with fragmentation.
            Each ROUTER can have a limit on the size of the packet. 
            So Internet layer fragments when sending the data.

    4. Network Access Layer:
        a. Logic Link Control (LLC),
            - The Logic Link Control layer (LLC) is in charge of adding information of which protocol on the Internet layer delivered data to be transmitted,
              so when receiving a frame from the network this layer on the receiving computer has to know to which protocol from the Internet layer it should deliver data.

        b. Media Access Control (MAC) and
            - The Media Access Control layer (MAC) is in charge of assembling the frame that will be sent over the network.
            - This layer is in charge of adding the source MAC address and the target MAC address

            - IMP: To communicate in an Ethernet network, MAC addresses are used.
                   Ethernet frames (packets) don't know anything about IP addresses. ARP is used to find out the MAC address of a host on the local network.

        c. Physical
            - Switches come in this layer.
        
1b.
How Internet Works
http://www.theshulers.com/whitepapers/internet_whitepaper/index.html

    1. The message would start at the top of the protocol stack on your computer and work it's way downward.

    2. If the message to be sent is long, each stack layer that the message passes through may break the message up into smaller chunks of data.
      This is because data sent over the Internet (and most computer networks) are sent in manageable chunks.
      On the Internet, these chunks of data are known as packets.

    3. The packets would go through the Application Layer and continue to the TCP layer.
       Each packet is assigned a port number.
       Ports will be explained later, but suffice to say that many programs may be using the TCP/IP stack and sending messages.
       We need to know which program on the destination computer needs to receive the message because it will be listening on a specific port.

    4. After going through the TCP layer, the packets proceed to the IP layer.
       This is where each packet receives it's destination address, 5.6.7.8.

    5. Now that our message packets have a port number and an IP address, they are ready to be sent over the Internet.
       The hardware layer takes care of turning our packets containing the alphabetic text of our message into electronic signals and transmitting them over the phone line.

    6. On the other end of the phone line your ISP has a direct connection to the Internet.
       The ISPs router examines the destination address in each packet and determines where to send it.
       Often, the packet's next stop is another router. More on routers and Internet infrastructure later.

    7. Eventually, the packets reach computer 5.6.7.8.
       Here, the packets start at the bottom of the destination computer's TCP/IP stack and work upwards.

    8. As the packets go upwards through the stack, all routing data that the sending computer's stack added (such as IP address and port number) is stripped from the packets.

    9. When the data reaches the top of the stack, the packets have been re-assembled into their original form, "Hello computer 5.6.7.8!"

1c.
TCP vs UDP
https://www.youtube.com/watch?v=Vdc8TCESIg8

    IMP:
    TRANSPORT LAYER:
        - Facilitates multiple applications to use the SAME network connection
        - It creates 65536 PORTS for a single connection
        - One application can also use multiple ports

        - Say Application 1 uses port 12347 to send a message to Port 80 on another machine
        - Application layer creates a message and sends to TRANSPORT LAYER.
        - Transport layer wraps the message with SOURCE PORT and DESTINATION port and creates a segment
        - It is then sent to Network Layer

    UDP Advantages:
        - UDP packet sizes are smaller.
        - UDP Header is 8 bytes while TCP is 20 bytes
        - More control over when data is sent out
            - A primitive form of Error Detection
        - Doesn't account for ORDERING of PACKING
        - A packet gets sent only once
        - Don't need to establish a connection
        - No congestion control
        Eg: Multimedia Streaming
            DNS Lookups
            UDP is used when speed is desirable and error correction isn’t necessary.
            For example, UDP is frequently used for live broadcasts and online games.

    TCP:
        - Can maintain ORDERING of packets
        - Congestion control
        Eg: Remote Access and File Transfers

1d.
Can two applications listen to the same port
http://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port

    For TCP, no. You can only have one application listening on a single port at one time. Now if you had 2 network cards, you could have one application listen on the first IP and the second one on the second IP using the same port number.

    For UDP (Multicasts), multiple applications can subscribe to the same port.

    It's not written in stone; but it's the way all APIs are written: the app opens a port, gets a handle to it, and the OS notifies it (via that handle) when a client connection (or a packet in UDP case) arrives.

    If the OS allowed two apps to open the same port, how would it know which one to notify?

    But... there are ways around it:
        As Jed noted, you could write a 'master' process, which would be the only one that really listens on the port and notifies others, using any logic it wants to separate client requests.

1e. Do browsers listen to port 80?
    They don't listen on port 80 they talk to port 80, or 443 if you're using SSL

    The browser will make the request from a random high-numbered port so more than browser can be active at the same time.

    In TCP/IP, a "session" must be unique and the session is defined as the 5-tuple (protocol, sourceIP, sourcePort, destinationIP, destinationPort). This allows the packets to be routed correctly on the internet.

    Typically when a client attempts to contact a server, it specifies 0 as its source port which means that the operating system assigns it an unused one. That means that the client will actually listen on that port rather than port 80.

    So you may get a session with the properties (TCP, mybox.com, 1101, www.microsoft.com, 80) when your browser goes out to access Microsoft's web pages.

    If you find you cannot bind your server to port 80, it will most likely because you already have a server running on that port, or your program doesn't have the required privileges to bind to that port (ports less than 1024 are generally considered privileged ports).

    Running netstat -a (on Linux or Windows) will tell you whether a server is bound to port 80. Look for a listener on port 80 (or http if it's resolving ports to service names), something like:
    tcp  0  0  localhost:http  *:*  LISTEN

1f. Do web browsers use different outgoing ports for different tabs?
https://serverfault.com/questions/296603/understanding-ports-how-do-multiple-browser-tabs-communicate-at-the-same-time
https://superuser.com/questions/1055281/do-web-browsers-use-different-outgoing-ports-for-different-tabs
    Yes, they do

2.
Why is MAC address required    
    Why do we need the MAC address if all Ethernet connections are done via IP address?
    - The MAC address is the hardware address, i.e. it is hard coded in the NIC of the machine.
      So it cannot be changed. Though all Ethernet communication happens via IP address, lower layers do not understand IP but they do understand MAC address.
      At the same time, there are instances when you use protocols other than IP, like IPX or AppleTalk.
      In such cases there has to be a mechanism which lets you work without changing hardware.
      So MAC addresses form the basic identifier of the hardware.
      MAC addresses are difficult to remember so we use IP addresses, which are more friendly, but not as friendly as names. 

3.
Switch vs Hub vs Router:
    - Switch:
        - Transmits frames to a particular destination PORT
        - Keeps tracks of mac address
        - Starts with an empty table and initially behaves like a Hub and learns as it goes

    - Hub:
        - Broadcasts frames to every devices connected
        - Doesn't keep track of MAC address

    - Router vs Switch
        - Router:
          - Connects network
          - Uses IP address
          - To send the packet to the next hop, you need its MAC address.
          While hopping through intermediate links, the IP address in the IP header don't change - only the MAC addresses change

        - Switch:
          - Creates network
          - Uses MAC address

4.
How NTLM Authentication Works
https://blogs.msdn.microsoft.com/chiranth/2013/09/20/ntlm-want-to-know-how-it-works/

    1. User uses a Username and Password to login to Client Machine
        - Client computes a HASH of the password 
        - Client discards the actual password
    2. Clients sends Username to Server
    3. Server computes a Random 16-byte Nounce (CHALLENGE) and sends it to client
    4. Client uses the HASH got in Step 1 to ENCRYPT the nounce
        - Client returns the Encrypted Nounce to the Server
    5. Server sends the following to DC
        - Encrypted Nounce
        - User name
        - Challenge sent to client
    6. DC uses
        - Username to get the HASH of the Actual Password
        - Uses the Above Hash to Encrypt the Challenge
        - Compares Encrypted Nounce with the one Computed
    7. If match success else failure

5.
How Kerberos Authentication Works
https://www.youtube.com/watch?v=kp5d8Yv3-0c
    
    3 PARTIES are Involved.
    1. Client
    2. Server
    3. KDC (Key Distribution Center) which is Active Directory Domain Controller (AD DC)

    In Kerberos NO communication between SERVER and KDC
    1. Client attempts to logon into the network.
        a. Client creates an AUTHENTICATOR with client's info. 
           It is valid for a short time
           IMP: This prevents from someone get hold of the AUTHENTICATOR and using it
        b. A portion of AUTHENTICATOR is unencrypted.
        c. A portion is ENCRYPTED using USER's PWD
           User's Pwd is used as an ENCRYPTION Key
        d. VERY IMP: PASSWORD is not sent. Instead it is used as a key

    2. Client sends the AUTHENTICATOR to KDC
    3. KDC sees the username and uses the PASSWORD that KDC has to decrpt the secured portion
       of the Authenticator
    4. If SUCCESS, then NO NEED for AUTHENTICATOR any more
    5. Use KDC's key and generate a TGC (Ticket Granting Ticket).
       Only KDC knows the key. So no one can decrypt it.
    6. Send the TGC to client.

    7. NOW CLIENTS wants to talk to Server
    8. Client will send TGC to KDC and ask for a TICKET for that server.
    9. KDC will use the TGC to get Client's info 
    10. KDC use the Server's KEY and generates a TICKET
        Only the SERVER can decrypt this TICKET as no one else is aware of the Server's key
    11. Client will store the TICKET and previously got TGC in the Kerberos Tray
    12. Clients uses the TICKET to talk to Server
    13. Client sends that TICKET evertime to the Server.
    14. So SERVER doesn't have to do anything.

https://www.youtube.com/watch?v=kp5d8Yv3-0c
    - Can be used only when DOMAIN is used
    - The client and the server should be on the same domain
    - If we directly use the IP address then the CLIENT does not go to the DC to resolve
      the Domain's IP address. Instead it goes directly to the Server and KDC does not
      come in the picture

    - Three Parties are involved
        1. Client (Has Client KEY)
        2. Server (Has Server KEY)
        3. Key Distribution Center KDC (Has Client Key, Server KEy and KDC Key)
           - In a AD network it is the AD DC
           - Brings everybody together
    - In Kerberos no communication between KDC and Server
    - Client takes the majority of processing burder in Kerberos

    1. Client constructs and Authenticator which contain basic info about the client.
        - This authenticator is GOOD only for a short time
        - A portion of the authenticator is UNENCRYPTED (USer name etc).
            - This will help KDC/DC in finding who the guy is
        - A portion is ENCRYPTED using USER'S PASSWORD
            - USer's password is used as ENCRYPTION KEY
            - So password need not be SENT over the network

    2. KDC receives the Authenticator from the Client.
        - KDC knows the Client's Password
        - KDC will use that password to try to decrypt the Authenticator
        - If FAILURE then incorrect password and reject it
        - If SUCCESS then the client is legitimate

        - Once Success THE AUTHENTICATOR is no longer needed.
        - KDC will generate a TICKET GRANTING TICKET using a KEY WHICH ONLY KDC is aware of
        - KDC will send the TGT to Client (TGT is valid for 8hrs in general)

    3. Client stores the TGT in MAIN MEMORY
        - So in case of Crash the TGT will go away
        - Next time client wants to talk to KDC it will directly use the TGT
        - This will help KDC in not doing a lookup of the client user name.
        - KDC can directly use its SPECIAL key to authenticate it.

    4. Now for Client to Communicate with the Server
        - Client will send TGT to KDC. 
        - Client will ask for a Ticket in order to TALK to Client

    5. KDC will know the PAssword of the File Server as even that is a member of the DC
        - DC will have the logon info the Server
        - KDC will use the SERVER's KEY and generate a TICKET
        - This ticket will be sent to the Client
        - Client can't use the Ticket as it doesn't know the Server's key

    6. Client will just send the TICKET got from KDC to the Server
        - Server will use its own KEY to see if the client is legitimate.
        - Server can prove that the client is legitimate as only KDC know the server's
          pasword.
        - Client will send the SERVER TICKET it got from KDC for every REQUEST that it
          sends to the SERVER
        - This way SERVER doesn't have to remember the TICKETS of the all the clients it is
          currently talking to.

https://technet.microsoft.com/en-us/library/cc780469(v=ws.10).aspx
    Possible Kerberos keys include:
    Long-term key.
    A key — known only to the target server and the KDC — with which the client’s ticket is encrypted.

    Client/server session key.
    A short-term, single-session key that is created by the KDC and used to encrypt the client-to-server and server-to-client messages after identity and authorization have been confirmed.

    KDC/user session key.
    The KDC and the user share a secret encryption key as well, which is used, for example, to encrypt the message to the client containing a session key.

http://sprabhu.blogspot.com/2013/04/ntlmauthentication-and-signing.html

5b.
NTLM vs Kerberos
http://sharepoint.stackexchange.com/questions/13400/ntlm-vs-kerberos

    NTLM is a properitary AuthN protocol invented by Microsoft whereas
    Kerberos is a standard protocol.

    The big difference is how the two protocols handle the authentication:
    NTLM uses a three-way handshake between the client and server and
    Kerberos uses a two-way handshake using a ticket granting service (key distribution center).
    In Kerberos the client must have access to a domain controller (which issues the tickets) whereas in
    NTLM the client contacts the server which contacts the domain controller.
    NTLM: Slower authentication because of pass-through authentication 
    Kerberos: Faster authentication because of unique ticketing system

6.
What happens when I click the Stop button on the browser
http://stackoverflow.com/questions/138116/what-happens-when-i-click-the-stop-button-on-the-browser?rq=1

    A Web Page load from a browser is usually a 4 step process (not considering redirections):

        - Browser sends HTTP Request, when the Server is available
        - Server executes code (for dynamic pages)
        - Server sends the HTTP Response (usually HTML)
        - Browser renders HTML, and asks for other files (images, css, ...)

    The browser reaction to "Stop" depends on the step your request is at that time:

        - If your server is slow or overloaded, and you hit "Stop" during step 1, nothing happens. The browser doesn't send the request.
        - Most of the times, however, "Stop" will be hit on steps 2, 3 and 4, and in those steps your code is already executed, the browser simply stops waiting for the response (2), or receiving the response (3), or rendering the response (4).

    The HTTP call itself is always a 2 steps action (Request/Response), and there is no automatic way to rollback the execution from the client

7.
what happens when you type in a URL in browser
https://github.com/alex/what-happens-when/blob/master/README.rst
http://stackoverflow.com/questions/2092527/what-happens-when-you-type-in-a-url-in-browser
http://igoro.com/archive/what-really-happens-when-you-navigate-to-a-url/

    In an extremely rough and simplified sketch, assuming the simplest possible HTTP request, no proxies, IPv4 and no problems in any step:

        - browser checks cache; if requested object is in cache and is fresh, skip to #9
        - browser asks OS for server's IP address
        - OS makes a DNS lookup and replies the IP address to the browser
        - browser opens a TCP connection to server (this step is much more complex with HTTPS)
        - browser sends the HTTP request through TCP connection
        - browser receives HTTP response and may close the TCP connection, or reuse it for another request
        - browser checks if the response is a redirect or a conditional response (3xx result status codes), authorization request (401), error (4xx and 5xx), etc.; these are handled differently from normal responses (2xx)
        - if cacheable, response is stored in cache
        - browser decodes response (e.g. if it's gzipped)
        - browser determines what to do with response (e.g. is it a HTML page, is it an image, is it a sound clip?)
        - browser renders response, or offers a download dialog for unrecognized types

Have you ever thought about what happens when you surf the web? It¿s not as simple as it seems:
    You type an URL into address bar in your preferred browser.
    The browser parses the URL to find the protocol, host, port, and path.
    It forms a HTTP request (that was most likely the protocol)
    To reach the host, it first needs to translate the human readable host into an IP number, and it does this by doing a DNS lookup on the host
    Then a socket needs to be opened from the user¿s computer to that IP number, on the port specified (most often port 80)
    When a connection is open, the HTTP request is sent to the host
    The host forwards the request to the server software (most often Apache) configured to listen on the specified port
    The server inspects the request (most often only the path), and launches the server plugin needed to handle the request (corresponding to the server language you use, PHP, Java, .NET, Python?)
    The plugin gets access to the full request, and starts to prepare a HTTP response.
    To construct the response a database is (most likely) accessed. A database search is made, based on parameters in the path (or data) of the request
    Data from the database, together with other information the plugin decides to add, is combined into a long string of text (probably HTML).
    The plugin combines that data with some meta data (in the form of HTTP headers), and sends the HTTP response back to the browser.
    The browser receives the response, and parses the HTML (which with 95% probability is broken) in the response
    A DOM tree is built out of the broken HTML
    New requests are made to the server for each new resource that is found in the HTML source (typically images, style sheets, and JavaScript files). Go back to step 3 and repeat for each resource.
    Stylesheets are parsed, and the rendering information in each gets attached to the matching node in the DOM tree
    Javascript is parsed and executed, and DOM nodes are moved and style information is updated accordingly
    The browser renders the page on the screen according to the DOM tree and the style information for each node
    You see the page on the screen
    You get annoyed the whole process was too slow.

8.
What is DHCP and how it works
http://whatismyipaddress.com/dhcp

    DHCP is at the heart of assigning you (and everyone) their IP address. The key word in DHCP is protocol—the guiding rules and process for Internet connections for everyone, everywhere. DHCP is consistent, accurate and works the same for every computer. Remember that without an IP address, you would not be able to receive the information you requested. As you've learned (by reading IP: 101), your IP address tells the Internet to send the information that you requested (Web page, email, data, etc.) right to the computer that requested it.

    THOSE INCREDIBLE PROTOCOLS
    There are more than one billion computers in the world, and each individual computer needs its own IP address whenever it's online. The TCP/IP protocols (our computers' built-in, internal networking software) include a DHCP protocol. It automatically assigns and keeps tabs of IP addresses and any "subnetworks" that require them. Nearly all IP addresses are dynamic, as opposed to "static" IP addresses that never change.

    DHCP is a part of the "application layer," which is just one of the several TCP/IP protocols. All of the processing and figuring out of what to send to whom happens virtually instantly.

    CLIENTS AND SERVERS
    The networking world classifies computers into two distinctive categories: 1) individual computers, called "hosts," and 2) computers that help process and send data (called "servers"). A DHCP server is one computer on the network that has a number of IP address at its disposal to assign to the computers/hosts on that network. If you use a cable company for Internet access, making them your Internet Service Provider, they likely are your DHCP server.

    PERMISSION SLIPS
    Think of getting an IP address as similar to obtaining a special permission slip from the DHCP server to use the Internet. In this scenario, you are the DHCP client—whenever you want to go on the Internet, your computer automatically requests an IP address from the network's DHCP server. If there's one available, the DHCP server sends a response containing an IP address to your computer.

    HOW DHCP WORKS
    The key word in DHCP is "dynamic." Because instead of having just one fixed and specific IP address, most computers will be assigned one that is available from a subnet or "pool" that is assigned to the network. The Internet isn't one big computer in one big location. It's an interconnected network of networks, all created to make one-on-one connections between any two clients that want to exchange information.

    One of the features of DHCP is that it provides IP addresses that "expire." When DHCP assigns an IP address, it actually leases that connection identifier to the user's computer for a specific amount of time. The default lease is five days.

    Here is how the DHCP process works when you go online:
        Your go on your computer to connect to the Internet.
        The network requests an IP address (this is actually referred to as a DHCP discover message).
        On behalf of your computer's request, the DHCP server allocates (leases) to your computer an IP address. This is referred to as the DHCP offer message.
        Your computer (remember—you're the DHCP client) takes the first IP address offer that comes along. It then responds with a DHCP request message that verifies the IP address that's been offered and accepted.
        DHCP then updates the appropriate network servers with the IP address and other configuration information for your computer.
        Your computer (or whatever network device you're using) accepts the IP address for the lease term.

    Typically, a DHCP server renews your lease automatically, without you (or even a network administrator) having to do anything. However, if that IP address's lease expires, you'll be assigned a new IP address using the same DHCP protocols.

9.
How WiFi (802.11) works
http://computer.howstuffworks.com/wireless-network.htm

    - A computer's wireless adapter translates data into a radio signal and transmits it using an antenna.
    - A wireless router receives the signal and decodes it. The router sends the information to the Internet using a physical, wired Ethernet connection.

10.
DNS and how it works
https://howdns.works/
http://dyn.com/blog/dns-why-its-important-how-it-works/

    - A directory of domain names and translate them to Internet Protocol (IP) addresses.
    - Information from all the domain name servers across the Internet are gathered together and housed at the Central Registry.
    - Host companies and Internet Service Providers interact with the Central Registry on a regular schedule to get updated DNS information. 

    When you visit a domain such as dyn.com, your computer follows a series of steps to turn the human-readable web address into a machine-readable IP address.
    This happens every time you use a domain name, whether you are viewing websites, sending email or listening to Internet radio stations like Pandora.

    Step 1: Request information
        - The process begins when you ask your computer to resolve a hostname, such as visiting http://dyn.com.
        - The first place your computer looks is its local DNS cache, which stores information that your computer has recently retrieved.
- 
        If your computer doesn’t already know the answer, it needs to perform a DNS query to find out.

    Step 2: Ask the recursive DNS servers
        - If the information is not stored locally, your computer queries (contacts) your ISP’s recursive DNS servers.
        - These specialized computers perform the legwork of a DNS query on your behalf.
        - Recursive servers have their own caches, so the process usually ends here and the information is returned to the user.

    Step 3: Ask the root nameservers
        - If the recursive servers don’t have the answer, they query the root nameservers.
        - A nameserver is a computer that answers questions about domain names, such as IP addresses.
        - The thirteen root nameservers act as a kind of telephone switchboard for DNS.
        - They don’t know the answer, but they can direct our query to someone that knows where to find it.

    Step 4: Ask the TLD nameservers
        - The root nameservers will look at the first part of our request, reading from right to left — www.dyn.com — and direct our query to the Top-Level Domain (TLD) nameservers for .com.
        - Each TLD, such as .com, .org, and .us, have their own set of nameservers, which act like a receptionist for each TLD.
        - These servers don’t have the information we need, but they can refer us directly to the servers that do have the information.

    Step 5: Ask the authoritative DNS servers
        - The TLD nameservers review the next part of our request — www.dyn.com — and direct our query to the nameservers responsible for this specific domain.
        - These authoritative nameservers are responsible for knowing all the information about a specific domain, which are stored in DNS records.
        - There are many types of records, which each contain a different kind of information.
        - In this example, we want to know the IP address for www.dyndns.com, so we ask the authoritative nameserver for the Address Record (A).

    Step 6: Retrieve the record
        - The recursive server retrieves the A record for dyn.com from the authoritative nameservers and stores the record in its local cache.
        - If anyone else requests the host record for dyn.com, the recursive servers will already have the answer and will not need to go through the lookup process again.
        - All records have a time-to-live value, which is like an expiration date.
        - After a while, the recursive server will need to ask for a new copy of the record to make sure the information doesn’t become out-of-date.

    Step 7: Receive the answer
        - Armed with the answer, recursive server returns the A record back to your computer.
        - Your computer stores the record in its cache, reads the IP address from the record, then passes this information to your browser.
        - The browser then opens a connection to the webserver and receives the website.

    This entire process, from start to finish, takes only milliseconds to complete.

11.
What is NTP and SNTP
http://www.ntp.org/ntpfaq/NTP-s-def.htm#AEN1422

    4.1.1. What is NTP?
    NTP stands for Network Time Protocol, and it is an Internet protocol used to synchronize the clocks of computers to some time reference. NTP is an Internet standard protocol originally developed by Professor David L. Mills at the University of Delaware.

    4.1.2. What is SNTP?
    SNTP (Simple Network Time Protocol) is basically also NTP, but lacks some internal algorithms that are not needed for all types of servers. See Q: 9.4. for more and detailed information.

    As a full implementation of the NTP protocol seemed too complicated for many systems, a simplified version of the protocol, namely SNTP had been defined.

    NTP needs some reference clock that defines the true time to operate. All clocks are set towards that true time. (It will not just make all systems agree on some time, but will make them agree upon the true time as defined by some standard.)

    NTP uses UTC as reference time.

12.
What is TLS and SSL

    Transport Layer Security (TLS) is a protocol that ensures privacy between communicating applications and their users on the Internet.
    When a server and client communicate, TLS ensures that no third party may eavesdrop or tamper with any message.
    TLS is the successor to the Secure Sockets Layer (SSL).

    SSL (Secure Sockets Layer) is the standard security technology for establishing an encrypted link between a web server and a browser.
    This link ensures that all data passed between the web server and browsers remain private and integral.

13.
What is DHCP

    - A protocol for configuring IP addresses
    - DHCP is a client server application.
    - It uses UDP ports
    - Client sends a BROADCAST message to find the DHCP server

14.
Sliding Window Protocol
http://computing.dcu.ie/~humphrys/Notes/Networks/data.sliding.html
https://www.techopedia.com/definition/869/sliding-window
https://www.youtube.com/watch?v=EHaSQBOrYDI

    Sliding window is a technique for controlling transmitted data packets between two network computers where reliable and sequential delivery of data packets is required, such as when using the Data Link Layer (OSI model) or Transmission Control Protocol (TCP).

    In the sliding window technique, each data packet (for most data link layers) and byte (in TCP) includes a unique consecutive sequence number, which is used by the receiving computer to place data in the correct order.
    The objective of the sliding window technique is to use the sequence numbers to avoid duplicate data and to request missing data.

15.
What is LDAP
http://stackoverflow.com/questions/239385/what-is-ldap-used-for

    Ldap Server + Ldap Protocol is a data store, or a database.
    It's not relational, but it's just a place to store data, and it's optimized to be efficient at reads more than writes.

    Lightweight Directory Access Protocol (LDAP) is a client/server protocol used to access and manage directory information. It reads and edits directories over IP networks and runs directly over TCP/IP using simple string formats for data transfer

    Why LDAP?
        When you have a task that requires ¿write/update once, read/query many times¿, you might consider using LDAP.
        LDAP is designed to provide extremely fast read/query performance for a large scale of dataset.
        Typically you want to store only a small piece of information for each entry.rThe add/delete/update performance is relatively slower compared with read/query because the assumption is that you don¿t do ¿update¿ that often.

        Imagine you have a website that has a million registered users with thousands of page requests per second.
        Without LDAP, every time users click a page, even for static page viewing, you will probably need to interact with your database to validate the user ID and its digital signature for this login session.
        Obviously, the query to your database for user-validation will become your bottleneck.
        By using LDAP, you can easily offload the user validation and gain significant performance improvement.
        Essentially, in this example, LDAP is another optimization layer outside your database to enhance performance, not replacing any database functions. 

16.
What is Samba
https://www.samba.org/samba/what_is_samba.html

    "Samba is an Open Source/Free Software suite that provides seamless file and print services to SMB/CIFS clients."
    Samba is freely available, unlike other SMB/CIFS implementations, and allows for interoperability between Linux/Unix servers and Windows-based clients.

17.
What is SMB
https://www.samba.org/cifs/docs/what-is-smb.html

    SMB is a protocol for sharing files, printers, serial ports, and communications abstractions such as named pipes and mail slots between computers. 

    Clients connect to servers using TCP/IP (actually NetBIOS over TCP/IP as specified in RFC1001 and RFC1002), NetBEUI or IPX/SPX. Once they have established a connection, clients can then send commands (SMBs) to the server that allow them to access shares, open files, read and write files, and generally do all the sort of things that you want to do with a file system.

AN EXAMPLE SMB EXCHANGE
     - The protocol elements (requests and responses) that clients and servers exchange are called SMBs.
     - They have a specific format that is very similar for both requests and responses. Each consists of a fixed size header portion, followed by a variable sized parameter and data portion.

     NEGOTIATE:
        - The client sends a negprot SMB to the server, listing the protocol dialects that it understands.
        - The server responds with the index of the dialect that it wants to use, or 0xFFFF if none of the dialects was acceptable.

     SESSION SETUP:
        - The client can proceed to logon to the server, if required.
        - They do this with a sesssetupX SMB.
        - The response indicates whether or not they have supplied a valid username password pair and if so, can provide additional information.
        - One of the most important aspects of the response is the UID of the logged on user.
        - This UID must be submitted with all subsequent SMBs on that connection to the server

     TREE CONNECT:
        - Once the client has logged on (and in older protocols-Core and CorePlus-you cannot logon), the client can proceed to connect to a tree.
        - The client sends a tcon or tconX SMB specifying the network name of the share that they wish to connect to, and if all is kosher, the server responds with a TID that the client will use in all future SMBs relating to that share.

     Having connected to a tree, the client can now open a file with an open SMB, followed by reading it with read SMBs, writing it with write SMBs, and closing it with close SMBs. 

18.
Ports vs Socket
http://stackoverflow.com/questions/152457/what-is-the-difference-between-a-port-and-a-socket

    Think of your machine as an apartment building:
        A port is an apartment number.
        A socket is the door of an apartment. 

    A socket represents a single connection between two network applications.
    A socket is one end-point of a two-way communication link between two programs running on the network
    SOCKET:
    - A network socket is an endpoint of a connection across a computer network.

    PORT:
    - A port represents an endpoint or "channel" for network communications.
      Port numbers allow different applications on the same computer to utilize network resources without interfering with each other.

      A port is part of the address in the TCP and UDP protocols.
      It is used to help the OS identify which application should get the data that is received.
      An OS has to support ports to support TCP and UDP because ports are an intrinsic part of TCP and UDP.

    A socket consists of three things:
        An IP address
        A transport protocol
        A port number

    A port is a number between 1 and 65535 inclusive that signifies a logical gate in a device. Every connection between a client and server requires a unique socket.

    For example:
        1030 is a port.
        (10.1.1.2 , TCP , port 1030) is a socket.

19. TCP Port and UDP Port
    An easy way to understand ports is to imagine your IP address is a cable box and the ports are the different channels on that cable box. The cable company knows how to send cable to your cable box based upon a unique serial number associated with that box (IP Address), and then you receive the individual shows on different channels (Ports).

    Ports work the same way. You have an IP address, and then many ports on that IP address. When I say many, I mean many. You can have a total of 65,535 TCP Ports and another 65,535 UDP ports. When a program on your computer sends or receives data over the Internet it sends that data to an ip address and a specific port on the remote computer, and receives the data on a usually random port on its own computer. If it uses the TCP protocol to send and receive the data then it will connect and bind itself to a TCP port. If it uses the UDP protocol to send and receive data, it will use a UDP port

20.
IPv6

Unlike the familiar IPv4 addresses which are 32 bits long, written in decimal, and separated by periods, IPv6 addresses are 128 bits long, written in hexadecimal, and separated by colons.

An example would be:
    3ffe:1900:4545:3:200:f8ff:fe21:67cf

Colons separate 16-bit fields. Leading zeros can be omitted in each field as can be seen above where the field :0003: is written :3:. In addition, a double colon (::) can be used once in an address to replace multiple fields of zeros. For example:

fe80:0:0:0:200:f8ff:fe21:67cf
can be written
fe80::200:f8ff:fe21:67cf

-----------------------------------------------
SECURITY
1. Asymmetric and Symmetric Encryption
https://www.youtube.com/watch?v=AQDCe585Lnc

Symmetric Encryption
    Encrypt and Decrypt using same key.
    How can the key be shared securely??

    Fast and less overload on the CPU
    
Asymmetric
    RSA Algorithm
    Everyone can know the Public Key
    Encrypt using Public Key.
    Can be decrypted only using the Private Key

2. What is SSL Termination
https://security.stackexchange.com/questions/30403/should-ssl-be-terminated-at-a-load-balancer

    Idea is to terminate the SSL at the load balancer and have unencrypted traffic within the data center (since the network in the data center should be trusted)

    An SSL connection sends encrypted data between an end-user's computer and web server by using a certificate for authentication. SSL termination is a process by which SSL-encrypted data traffic is decrypted (or offloaded).

    SSL termination at load balancer is desired because decryption is resource and CPU intensive. Putting the decryption burden on the load balancer enables the server to spend processing power on application tasks, which helps improve performance.

    SSL/TLS trust should terminate at the SSL offloading device since the department that manages that device often also manages the networking and infrastructure. There is a certain amount of contractual trust there. There is no point of encrypting data at a downstream server since the same people who are supporting the network usually have access to this as well.

-----------------------------------------------
MISC
1.
What is REST Servcie

A REST API defines a set of functions which developers can perform requests and receive responses via HTTP protocol such as GET and POST.

An API can be considered “RESTful” if it has the following features (not a complete list just the main ones):
    - Client–server – The client handles the front end the server handles the backend and can both be replaced independently of each other.
    - Stateless – No client data is stored on the server between requests and session state is stored on the client.
    - Cacheable – Clients can cache response (just like browsers caching static elements of a web page) to improve performance.

1b.
GET vs POST
http://stackoverflow.com/questions/11229206/difference-between-get-and-post-method-in-comparision-with-http-and-rest

    GET should be used to retrieve a resource. This operation should be idempotent, meaning it should not change any state on the server.

    POST should be used to add new information to the server. This is usually performed on a URL that represents a "container" of resources. The POST will add a new resource to this container.

    PUT should be used to update an existing resource.

    DELETE should be obvious.

1c. RPC vs REST
https://stackoverflow.com/questions/26830431/web-service-differences-between-rest-and-rpc
https://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/

    Remote Procedure Call (RPC) is way to describe a mechanism that lets you call a procedure in another process and exchange data by message passing. It typically involves generating some method stubs on the client process that makes making the call appear local, but behind the stub is logic to marshall the request and send it to the server process. The server process then unmarshalls the request and invokes the desired method before repeating the process in reverse to get whatever the method returns back to the client. HTTP is sometimes used as the underlying protocol for message passing, but nothing about RPC is inherently bound to HTTP. Remote Method Invocation (RMI) is closely related to RPC, but it takes remote invocation a step further by making it object oriented and providing the capability to keep references to remote objects and invoke their methods.

1d. RESTful API is stateless
https://stackoverflow.com/questions/34130036/how-to-understand-restful-api-is-stateless

    each request from client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client

1e. What exactly is RESTful programming?
https://stackoverflow.com/questions/671118/what-exactly-is-restful-programming

    An architectural style called REST (Representational State Transfer) advocates that web applications should use HTTP as it was originally envisioned. Lookups should use GET requests. PUT, POST, and DELETE requests should be used for mutation, creation, and deletion respectively.

    REST proponents tend to favor URLs, such as

    http://myserver.com/catalog/item/1729
    but the REST architecture does not require these "pretty URLs". A GET request with a parameter
    http://myserver.com/catalog?item=1729
    is every bit as RESTful.

    Keep in mind that GET requests should never be used for updating information. For example, a GET request for adding an item to a cart
    http://myserver.com/addToCart?cart=314159&item=1729

1f. IIOP
https://whatis.techtarget.com/definition/IIOP-Internet-Inter-ORB-Protocol

    IIOP (Internet Inter-ORB Protocol) is a protocol that makes it possible for distributed programs written in different programming languages to communicate over the Internet. IIOP is a critical part of a strategic industry standard, the Common Object Request Broker Architecture (CORBA). Using CORBA's IIOP and related protocols, a company can write programs that will be able to communicate with their own or other company's existing or future programs wherever they are located and without having to understand anything about the program other than its service and a name. CORBA and IIOP are competing with a similar strategy from Microsoft called the Distributed Component Object Model (DCOM).

1g. RMI
    RMI uses a network-based registry to keep track of the distributed objects. The server object makes a method available for remote invocation by binding it to a name in the registry. The client object, in turn, can check for availability of an object by looking up its name in the registry . The registry acts as a limited central management point for RMI . The registry is simply a name repository . It does not address the problem of actually invoking the remote method

2.
What is a Websocket
http://stackoverflow.com/questions/2681267/what-is-the-fundamental-difference-between-websockets-and-pure-tcp

    It's easier to communicate via TCP sockets when you're working within an intranet boundary, since you likely have control over the machines on that network and can open ports suitable for making the TCP connections.

    Over the internet, you're communicating with someone else's server on the other end. They are extremely unlikely to have any old socket open for connections. Usually they will have only a few standard ones such as port 80 for HTTP or 443 for HTTPS. So, to communicate with the server you are obliged to connect using one of those ports.

    Given that these are standard ports for web servers that generally speak HTTP, you're therefore obliged to conform to the HTTP protocol, otherwise the server won't talk to you. The purpose of web sockets is to allow you to initiate a connection via HTTP, but then negotiate to use the web sockets protocol (assuming the server is capable of doing so) to allow a more "TCP socket"-like communication stream.

3. Relational Database vs NoSQL Database
    RDBMS
    - Scale up: By increasing the capacity of the box
    - For storing Structured Data
        - Already mention the number of rows and columns; type of data can go etc
    - Can handle atomic transactions

    - Examples
        - Oracle
        - Microsoft SQL Server
        - DB2
        - MySql
        - PostGreSql
        - MS Access

    NoSql:
    - Scale Out: By adding more boxes
    - For storing semi-structured data
    - Cannot handle atomic transactions
    - Eventual Consistency
    - Can handle Structured and Unstructured data

    WHEN:
        - don't have to store relationship between elements

        1. Key Value Store
            - Memcached
            - Coherence
            - Redis

        2. Tabular
            - BigTable
            - Hbase
            - Accumulo

        3. Document Oriented
            - MongoDB
            - CouchDB
            - Cloudant

4. MongoDB:
    - Data is stored in the form of JSON style objects called BSON (Binary JSON - Binary Javascript Object Notation)
    For example, consider the following JSON document:
      {
       "name" : "MongoDB",
       "type" : "database",
       "count" : 1,
       "versions": [ "v3.2", "v3.0", "v2.6" ],
       "info" : { x : 203, y : 102 }
      }

     Document doc = new Document("name", "MongoDB")
                    .append("type", "database")
                    .append("count", 1)
                    .append("versions", Arrays.asList("v3.2", "v3.0", "v2.6"))
                    .append("info", new Document("x", 203).append("y", 102));

4.
ACID Properties in DB
https://en.wikipedia.org/wiki/ACID

    Atomicity
        Atomicity requires that each transaction be "all or nothing".
        If one part of the transaction fails, then the entire transaction fails, and the database state is left unchanged.
        An atomic system must guarantee atomicity in each and every situation, including power failures, errors, and crashes.
        To the outside world, a committed transaction appears (by its effects on the database) to be indivisible ("atomic"), and an aborted transaction does not happen.

    Consistency
        The consistency property ensures that any transaction will bring the database from one valid state to another.
        Any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof.
        This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined rules.

    Isolation
        The isolation property ensures that the concurrent execution of transactions results in a system state that would be obtained if transactions were executed serially.
        i.e., one after the other. Providing isolation is the main goal of concurrency control. Depending on the concurrency control method (i.e., if it uses strict - as opposed to relaxed - serializability), the effects of an incomplete transaction might not even be visible to another transaction.

    Durability
        The durability property ensures that once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors.
        Once a group of SQL statements execute, the results need to be stored permanently (even if the database crashes immediately thereafter).
        To defend against power loss, transactions (or their effects) must be recorded in a non-volatile memory.

5. BASE Properties
    A BASE system gives up on consistency.

    Basically available indicates that the system does guarantee availability, in terms of the CAP theorem.
    Soft state indicates that the state of the system may change over time, even without input. This is because of the eventual consistency model.
    Eventual consistency indicates that the system will become consistent over time, given that the system doesn't receive input during that time.

6.
Tabs vs Spaces as Coding Style
http://softwareengineering.stackexchange.com/questions/57/tabs-versus-spaces-what-is-the-proper-indentation-character-for-everything-in-e

    - Use tabs to indent the start of the line, one tab per indent level, and let everyone pick how wide they want that to be.

    - Use spaces if you're lining up characters within a line, so they always line up regardless of tab size.

    SPACES:
        - Consistent across computers
        - Tabs are not originally meant for indentation, they are meant for tabulation, and do a dreadful job at it

    TABS:
        - Plus a lot of programmers spend a significant amount of time writing html, css, etc, were the fact that a file with 4 spaces instead of tabs is 5-15% larger is actually significant.
        - Example: this page you are on right now loaded at least 10% slower than it should have because the SE team used spaces.
--------------------------------------------------------------------------------
DISTRIBUTED SYSTEMS:
1. Docker and Kubernetes
https://blog.containership.io/k8svsdocker
    At their core, containers are a way of packaging software. What makes them special is that when you run a container, you know exactly how it will run - it¿s predictable, repeatable and immutable. There are no unexpected errors when you move it to a new machine, or between environments. All of your application¿s code, libraries, and dependencies are packed together in the container as an immutable artifact. You can think of running a container like running a virtual machine, without the overhead of spinning up an entire operating system. For this reason, bundling your application in a container vs. a virtual machine will improve startup time significantly.

    Docker:
        Once you¿ve recovered from the excitement of spinning up your first few Docker containers, you¿ll realize that something is missing. 
        If you want to run multiple containers across multiple machines - which you¿ll need to do if you¿re using microservices - there is still a lot of work to do.

        You need to start the right containers at the right time, figure out how they can talk to each other, handle storage considerations, and deal with failed containers or hardware. Doing all of this manually would be a nightmare. 
        Luckily, that¿s where Kubernetes comes in.

    Kubernetes:
        Kubernetes is an open source container orchestration platform, allowing large numbers of containers to work together in harmony, reducing operational burden. It helps with things like:

            Running containers across many different machines
            Scaling up or down by adding or removing containers when demand changes
            Keeping storage consistent with multiple instances of an application
            Distributing load between the containers
            Launching new containers on different machines if something fails 

    When used together, both Docker and Kubernetes are great tools for developing a modern cloud architecture, but they are fundamentally different at their core. It is important to understand the high-level differences between the technologies when building your stack.

2. Memcache, Redis
https://stackoverflow.com/questions/10558465/memcached-vs-redis
    Redis is more powerful, more popular, and better supported than memcached. Memcached can only do a small fraction of the things Redis can do. Redis is better even where their features overlap.

    For anything new, use Redis.

    Both tools are powerful, fast, in-memory data stores that are useful as a cache. Both can help speed up your application by caching database results, HTML fragments, or anything else that might be expensive to generate.

    Redis is what is called a key-value store, often referred to as a NoSQL database
    In-memory key-value store, originally intended for caching
    In-memory data structure store, used as database, cache and message broker

3. Hadoop
    Hadoop is an open-source software framework for storing data and running applications on clusters of commodity hardware.

    Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data. Its distributed file system facilitates rapid data transfer rates among nodes and allows the system to continue operating in case of a node failure. This approach lowers the risk of catastrophic system failure and unexpected data loss, even if a significant number of nodes become inoperative. Consequently, Hadoop quickly emerged as a foundation for big data processing tasks, such as scientific analytics, business and sales planning, and processing enormous volumes of sensor data, including from internet of things sensors.

4. Apache Spark
    Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets

4b. Spark vs MapReduce
https://stackoverflow.com/questions/32572529/why-is-spark-faster-than-hadoop-map-reduce

    Spark uses "lazy evaluation" to form a directed acyclic graph (DAG) of consecutive computation stages. In this way, the execution plan can be optimized, e.g. to minimize shuffling data around. In contrast, this should be done manually in MapReduce by tuning each MR step. (It would be easier to understand this point if you are familiar with the execution plan optimization in RDBMS or the DAG-style execution of Apache Tez)
    Spark ecosystem has established a versatile stack of components to handle SQL, ML, Streaming, Graph Mining tasks. But in the hadoop ecosystem you have to install other packages to do these individual things.

    One of the main limitations of MapReduce is that it persists the full dataset to HDFS after running each job. This is very expensive, because it incurs both three times (for replication) the size of the dataset in disk I/O and a similar amount of network I/O. Spark takes a more holistic view of a pipeline of operations. When the output of an operation needs to be fed into another operation, Spark passes the data directly without writing to persistent storage. This is an innovation over MapReduce that came from Microsoft's Dryad paper, and is not original to Spark.

    The main innovation of Spark was to introduce an in-memory caching abstraction. This makes Spark ideal for workloads where multiple operations access the same input data. Users can instruct Spark to cache input data sets in memory, so they don't need to be read from disk for each operation.

    What about Spark jobs that would boil down to a single MapReduce job? In many cases also these run faster on Spark than on MapReduce. The primary advantage Spark has here is that it can launch tasks much faster. MapReduce starts a new JVM for each task, which can take seconds with loading JARs, JITing, parsing configuration XML, etc. Spark keeps an executor JVM running on each node, so launching a task is simply a matter of making an RPC to it and passing a Runnable to a thread pool, which takes in the single digits of milliseconds.

    Lastly, a common misconception probably worth mentioning is that Spark somehow runs entirely in memory while MapReduce does not. This is simply not the case. Spark's shuffle implementation works very similarly to MapReduce's: each record is serialized and written out to disk on the map side and then fetched and deserialized on the reduce side.

5. Hive

6. AWS
6a. Amazon EC2 - Elastic Compute Cloud
    Can create virtual servers
    We can tell how powerful these servers should be
    Can choose the region where you want the virtual server to be

7. Kafka
    Distributed Event Steaming Message PubSub system
    
8. RabbitMQ

9. Zookeeper
    Distributed Configuration Management system




